#!/usr/bin/env python3
"""
æ£€æŸ¥ Vision Cache å‚æ•°å’Œå®Œæ•´æ€§

ç”¨æ³•:
    python check_vision_cache.py [--cache-dir PATH] [--dataset NAME] [--detailed]
"""

import argparse
import os
import sys
import json
from pathlib import Path
from collections import defaultdict
import torch
from tqdm import tqdm


def format_bytes(bytes_size):
    """æ ¼å¼åŒ–å­—èŠ‚å¤§å°"""
    for unit in ['B', 'KB', 'MB', 'GB', 'TB']:
        if bytes_size < 1024.0:
            return f"{bytes_size:.2f} {unit}"
        bytes_size /= 1024.0
    return f"{bytes_size:.2f} PB"


def check_cache_file(cache_path):
    """æ£€æŸ¥å•ä¸ªç¼“å­˜æ–‡ä»¶çš„å‚æ•°"""
    try:
        data = torch.load(cache_path, map_location='cpu')

        info = {
            'video_id': data.get('video_id', 'N/A'),
            'video_path': data.get('video_path', 'N/A'),
            'nframe': data.get('nframe', 'N/A'),
            'dataset': data.get('dataset', 'N/A'),
            'model_signature': data.get('model_signature', 'N/A'),
            'dtype': data.get('dtype', 'N/A'),
            'shape': data.get('shape', 'N/A'),
            'file_size': os.path.getsize(cache_path),
        }

        # æ£€æŸ¥ vision_features çš„å®é™… dtype å’Œ shape
        if 'vision_features' in data:
            features = data['vision_features']
            info['actual_dtype'] = str(features.dtype)
            info['actual_shape'] = list(features.shape)

        return info, None

    except Exception as e:
        return None, str(e)


def check_cache_directory(cache_dir, dataset_name=None, detailed=False):
    """æ£€æŸ¥ç¼“å­˜ç›®å½•"""
    cache_dir = Path(cache_dir)

    if not cache_dir.exists():
        print(f"âŒ ç¼“å­˜ç›®å½•ä¸å­˜åœ¨: {cache_dir}")
        return

    print("=" * 80)
    print("Vision Cache å‚æ•°æ£€æŸ¥")
    print("=" * 80)
    print(f"ç¼“å­˜ç›®å½•: {cache_dir}")
    print()

    # æ£€æŸ¥å…ƒæ•°æ®æ–‡ä»¶
    meta_file = cache_dir / "cache_meta.json"
    if meta_file.exists():
        print("ğŸ“„ å…ƒæ•°æ®æ–‡ä»¶ (cache_meta.json):")
        print("-" * 80)
        with open(meta_file, 'r') as f:
            metadata = json.load(f)
        print(json.dumps(metadata, indent=2, ensure_ascii=False))
        print()
    else:
        print("âš ï¸  å…ƒæ•°æ®æ–‡ä»¶ä¸å­˜åœ¨")
        metadata = None

    # æ‰«ææ•°æ®é›†ç›®å½•
    dataset_dirs = [d for d in cache_dir.iterdir() if d.is_dir()]

    if dataset_name:
        dataset_dirs = [d for d in dataset_dirs if d.name == dataset_name]
        if not dataset_dirs:
            print(f"âŒ æ•°æ®é›†ä¸å­˜åœ¨: {dataset_name}")
            return

    if not dataset_dirs:
        print("âš ï¸  æ²¡æœ‰æ‰¾åˆ°æ•°æ®é›†ç¼“å­˜ç›®å½•")
        return

    print("=" * 80)
    print("æ•°æ®é›†ç¼“å­˜æ£€æŸ¥")
    print("=" * 80)
    print()

    for dataset_dir in sorted(dataset_dirs):
        print(f"ğŸ“ æ•°æ®é›†: {dataset_dir.name}")
        print("-" * 80)

        # æŸ¥æ‰¾æ‰€æœ‰ç¼“å­˜æ–‡ä»¶
        cache_files = list(dataset_dir.glob("*.pt"))

        if not cache_files:
            print("  âš ï¸  æ²¡æœ‰æ‰¾åˆ°ç¼“å­˜æ–‡ä»¶")
            print()
            continue

        print(f"  ç¼“å­˜æ–‡ä»¶æ•°: {len(cache_files)}")

        # ç»Ÿè®¡ä¿¡æ¯
        stats = {
            'total_files': len(cache_files),
            'total_size': 0,
            'nframes': defaultdict(int),
            'model_signatures': defaultdict(int),
            'dtypes': defaultdict(int),
            'shapes': defaultdict(int),
            'errors': [],
        }

        # æ£€æŸ¥æ¯ä¸ªæ–‡ä»¶
        print(f"  æ­£åœ¨æ£€æŸ¥ç¼“å­˜æ–‡ä»¶...")

        sample_info = None

        for cache_file in tqdm(cache_files, desc="  æ‰«æ", disable=not detailed):
            info, error = check_cache_file(cache_file)

            if error:
                stats['errors'].append((cache_file.name, error))
                continue

            if info:
                # ä¿å­˜ç¬¬ä¸€ä¸ªæ ·æœ¬ä½œä¸ºç¤ºä¾‹
                if sample_info is None:
                    sample_info = info

                # ç»Ÿè®¡
                stats['total_size'] += info['file_size']
                stats['nframes'][info['nframe']] += 1
                stats['model_signatures'][info['model_signature']] += 1
                stats['dtypes'][info.get('actual_dtype', info['dtype'])] += 1

                shape_str = str(info.get('actual_shape', info['shape']))
                stats['shapes'][shape_str] += 1

        # è¾“å‡ºç»Ÿè®¡ä¿¡æ¯
        print()
        print(f"  âœ“ æˆåŠŸè¯»å–: {stats['total_files'] - len(stats['errors'])} ä¸ªæ–‡ä»¶")
        print(f"  âœ— è¯»å–å¤±è´¥: {len(stats['errors'])} ä¸ªæ–‡ä»¶")
        print(f"  ğŸ“Š æ€»å¤§å°: {format_bytes(stats['total_size'])}")
        print()

        # å¹³å‡æ–‡ä»¶å¤§å°
        if stats['total_files'] > 0:
            avg_size = stats['total_size'] / stats['total_files']
            print(f"  å¹³å‡æ–‡ä»¶å¤§å°: {format_bytes(avg_size)}")
            print()

        # nframe åˆ†å¸ƒ
        print(f"  ğŸ“ˆ nframe åˆ†å¸ƒ:")
        for nframe, count in sorted(stats['nframes'].items()):
            pct = count / stats['total_files'] * 100
            print(f"    {nframe} å¸§: {count} ä¸ªæ–‡ä»¶ ({pct:.1f}%)")
        print()

        # æ¨¡å‹ç­¾ååˆ†å¸ƒ
        print(f"  ğŸ”‘ æ¨¡å‹ç­¾ååˆ†å¸ƒ:")
        for sig, count in sorted(stats['model_signatures'].items()):
            pct = count / stats['total_files'] * 100
            print(f"    {sig}: {count} ä¸ªæ–‡ä»¶ ({pct:.1f}%)")
        print()

        # dtype åˆ†å¸ƒ
        print(f"  ğŸ”¢ æ•°æ®ç±»å‹åˆ†å¸ƒ:")
        for dtype, count in sorted(stats['dtypes'].items()):
            pct = count / stats['total_files'] * 100
            print(f"    {dtype}: {count} ä¸ªæ–‡ä»¶ ({pct:.1f}%)")
        print()

        # shape åˆ†å¸ƒ
        print(f"  ğŸ“ ç‰¹å¾å½¢çŠ¶åˆ†å¸ƒ:")
        for shape, count in sorted(stats['shapes'].items()):
            pct = count / stats['total_files'] * 100
            print(f"    {shape}: {count} ä¸ªæ–‡ä»¶ ({pct:.1f}%)")
        print()

        # æ˜¾ç¤ºç¤ºä¾‹
        if sample_info and detailed:
            print(f"  ğŸ“‹ ç¼“å­˜æ–‡ä»¶ç¤ºä¾‹:")
            print(f"    Video ID: {sample_info['video_id']}")
            print(f"    Video Path: {sample_info['video_path']}")
            print(f"    nframe: {sample_info['nframe']}")
            print(f"    Dataset: {sample_info['dataset']}")
            print(f"    Model Signature: {sample_info['model_signature']}")
            print(f"    Dtype: {sample_info.get('actual_dtype', sample_info['dtype'])}")
            print(f"    Shape: {sample_info.get('actual_shape', sample_info['shape'])}")
            print(f"    File Size: {format_bytes(sample_info['file_size'])}")
            print()

        # æ˜¾ç¤ºé”™è¯¯
        if stats['errors'] and detailed:
            print(f"  âŒ é”™è¯¯æ–‡ä»¶:")
            for filename, error in stats['errors'][:10]:  # åªæ˜¾ç¤ºå‰10ä¸ª
                print(f"    {filename}: {error}")
            if len(stats['errors']) > 10:
                print(f"    ... è¿˜æœ‰ {len(stats['errors']) - 10} ä¸ªé”™è¯¯")
            print()

        # ä¸€è‡´æ€§æ£€æŸ¥
        print(f"  âœ… ä¸€è‡´æ€§æ£€æŸ¥:")

        # æ£€æŸ¥ nframe æ˜¯å¦ä¸€è‡´
        if len(stats['nframes']) == 1:
            print(f"    âœ“ nframe ä¸€è‡´: {list(stats['nframes'].keys())[0]} å¸§")
        else:
            print(f"    âš ï¸  nframe ä¸ä¸€è‡´! å‘ç° {len(stats['nframes'])} ç§ä¸åŒçš„å€¼:")
            for nframe, count in sorted(stats['nframes'].items()):
                print(f"       {nframe} å¸§: {count} ä¸ªæ–‡ä»¶")

        # æ£€æŸ¥æ¨¡å‹ç­¾åæ˜¯å¦ä¸€è‡´
        if len(stats['model_signatures']) == 1:
            print(f"    âœ“ æ¨¡å‹ç­¾åä¸€è‡´: {list(stats['model_signatures'].keys())[0]}")
        else:
            print(f"    âš ï¸  æ¨¡å‹ç­¾åä¸ä¸€è‡´! å‘ç° {len(stats['model_signatures'])} ç§ä¸åŒçš„å€¼")

        # æ£€æŸ¥ dtype æ˜¯å¦ä¸€è‡´
        if len(stats['dtypes']) == 1:
            print(f"    âœ“ æ•°æ®ç±»å‹ä¸€è‡´: {list(stats['dtypes'].keys())[0]}")
        else:
            print(f"    âš ï¸  æ•°æ®ç±»å‹ä¸ä¸€è‡´! å‘ç° {len(stats['dtypes'])} ç§ä¸åŒçš„å€¼")

        # æ£€æŸ¥ shape æ˜¯å¦ä¸€è‡´
        if len(stats['shapes']) <= 2:  # å…è®¸å°‘é‡å·®å¼‚ï¼ˆè§†é¢‘é•¿åº¦å¯èƒ½ä¸åŒï¼‰
            print(f"    âœ“ ç‰¹å¾å½¢çŠ¶åŸºæœ¬ä¸€è‡´")
        else:
            print(f"    âš ï¸  ç‰¹å¾å½¢çŠ¶å·®å¼‚è¾ƒå¤§! å‘ç° {len(stats['shapes'])} ç§ä¸åŒçš„å½¢çŠ¶")

        print()
        print()

    # æœ€ç»ˆæ€»ç»“
    print("=" * 80)
    print("æ€»ç»“")
    print("=" * 80)

    if metadata:
        total_samples = metadata.get('total_samples', 0)
        total_size_bytes = metadata.get('total_size_bytes', 0)
        print(f"âœ“ å…ƒæ•°æ®è®°å½•çš„æ ·æœ¬æ•°: {total_samples}")
        print(f"âœ“ å…ƒæ•°æ®è®°å½•çš„æ€»å¤§å°: {format_bytes(total_size_bytes)}")

    print(f"âœ“ æ‰«æçš„æ•°æ®é›†æ•°: {len(dataset_dirs)}")

    print()
    print("æ£€æŸ¥å®Œæˆ!")
    print("=" * 80)


def main():
    parser = argparse.ArgumentParser(description='æ£€æŸ¥ Vision Cache å‚æ•°å’Œå®Œæ•´æ€§')
    parser.add_argument('--cache-dir', default='/disk3/minami/LMUData/vision_cache',
                       help='ç¼“å­˜ç›®å½• (é»˜è®¤: /disk3/minami/LMUData/vision_cache)')
    parser.add_argument('--dataset', default=None,
                       help='åªæ£€æŸ¥ç‰¹å®šæ•°æ®é›† (å¯é€‰)')
    parser.add_argument('--detailed', action='store_true',
                       help='æ˜¾ç¤ºè¯¦ç»†ä¿¡æ¯ï¼ˆåŒ…æ‹¬ç¤ºä¾‹å’Œé”™è¯¯ï¼‰')

    args = parser.parse_args()

    check_cache_directory(args.cache_dir, args.dataset, args.detailed)


if __name__ == '__main__':
    main()
