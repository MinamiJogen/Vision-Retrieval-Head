"""
LongVA æ¨¡å‹ - æ”¯æŒ Vision ç‰¹å¾ç¼“å­˜
é€šè¿‡ç¼“å­˜ vision_tower è¾“å‡ºï¼Œé¿å…é‡å¤è®¡ç®— vision encoder
mm_projector å’Œ LLM æ­£å¸¸è¿è¡Œï¼Œç¡®ä¿æµç¨‹å®Œå…¨ä¸€è‡´
"""

import torch
from PIL import Image
import numpy as np
import warnings
import sys
import os
from pathlib import Path
import hashlib

warnings.filterwarnings("ignore", message=".*copying from a non-meta parameter.*")

from .base import BaseModel


class LongVA_Cached(BaseModel):
    """
    LongVA model wrapper with vision feature caching support.

    ç¼“å­˜ vision_tower çš„è¾“å‡ºï¼Œmm_projector å’Œ LLM æ­£å¸¸è¿è¡Œã€‚
    ç¡®ä¿é™¤ vision_tower å¤–çš„æµç¨‹ä¸åŸå§‹ LongVA å®Œå…¨ä¸€è‡´ã€‚

    ä½¿ç”¨æ–¹æ³•:
        1. å…ˆè¿è¡Œ preprocess_video_mme.py é¢„å¤„ç†æ•°æ®é›†
        2. åœ¨ config.py ä¸­é…ç½®:
           "LongVA-7B-Cached": partial(
               LongVA_Cached,
               model_path="lmms-lab/LongVA-7B-DPO",
               cache_dir="/disk3/minami/LMUData/vision_cache"
           )
    """

    INSTALL_REQ = False
    INTERLEAVE = True
    VIDEO_LLM = True

    def __init__(self, model_path="lmms-lab/LongVA-7B",
                 cache_dir=None,
                 enable_cache=True,
                 **kwargs):
        try:
            from longva.model.builder import load_pretrained_model
            from longva.mm_utils import tokenizer_image_token, process_images
            from longva.constants import IMAGE_TOKEN_INDEX
        except ImportError:
            raise ImportError(
                "LongVA is not installed. Please install it from the LongVA repository."
            )

        self.model_path = model_path
        self.tokenizer_image_token = tokenizer_image_token
        self.process_images = process_images
        self.IMAGE_TOKEN_INDEX = IMAGE_TOKEN_INDEX

        # ç¼“å­˜é…ç½®
        self.enable_cache = enable_cache and cache_dir is not None
        self.cache_dir = Path(cache_dir) if cache_dir else None
        self.cache_hits = 0
        self.cache_misses = 0

        # åŠ è½½æ¨¡å‹
        print(f"åŠ è½½æ¨¡å‹: {model_path}")
        self.tokenizer, self.model, self.image_processor, _ = load_pretrained_model(
            model_path, None, "llava_qwen", device_map="auto"
        )
        self.model.eval()

        # ç”Ÿæˆæ¨¡å‹ç­¾å
        self.model_signature = self._generate_model_signature()

        # é»˜è®¤ç”Ÿæˆé…ç½®
        self.gen_kwargs = {
            "do_sample": False,
            "temperature": 0,
            "top_p": None,
            "num_beams": 1,
            "use_cache": True,
            "max_new_tokens": 1024,
        }

        # è§†é¢‘é…ç½®
        self.nframe = kwargs.get('nframe', 128)  # é»˜è®¤ 128 å¸§ï¼Œä¸é¢„å¤„ç†ä¸€è‡´
        self.fps = kwargs.get('fps', -1)

        torch.cuda.empty_cache()

        if self.enable_cache:
            print(f"âœ“ Vision ç¼“å­˜å·²å¯ç”¨: {self.cache_dir}")
            print(f"âœ“ æ¨¡å‹ç­¾å: {self.model_signature}")
            print(f"âœ“ ç¼“å­˜æ¨¡å¼: float16 æ— å‹ç¼©")

    def _generate_model_signature(self) -> str:
        """ç”Ÿæˆæ¨¡å‹ç­¾åï¼ˆåŸºäº vision tower é…ç½®ï¼‰"""
        config = self.model.config
        sig_str = f"{config.mm_vision_tower}:layer{config.mm_vision_select_layer}"
        return hashlib.md5(sig_str.encode()).hexdigest()[:8]

    def _get_cache_key(self, video_id: str, dataset: str, nframe: int) -> str:
        """ç”Ÿæˆç¼“å­˜é”®"""
        key_str = f"{dataset}:{video_id}:nframe{nframe}:model{self.model_signature}"
        return hashlib.md5(key_str.encode()).hexdigest()

    def _get_cache_path(self, cache_key: str, dataset: str) -> Path:
        """è·å–ç¼“å­˜æ–‡ä»¶è·¯å¾„"""
        dataset_dir = self.cache_dir / dataset
        return dataset_dir / f"{cache_key}.pt"

    def _load_cached_vision_features(self, video_path: str, dataset: str,
                                     nframe: int) -> torch.Tensor:
        """åŠ è½½ç¼“å­˜çš„ vision_tower è¾“å‡º"""
        if not self.enable_cache:
            return None

        video_id = Path(video_path).stem
        cache_key = self._get_cache_key(video_id, dataset, nframe)
        cache_path = self._get_cache_path(cache_key, dataset)

        if not cache_path.exists():
            return None

        try:
            data = torch.load(cache_path, map_location='cpu')
            vision_features = data['vision_features']

            # ç§»åŠ¨åˆ°è®¾å¤‡å¹¶ä¿æŒ float16
            vision_features = vision_features.to(self.model.device, dtype=torch.float16)

            return vision_features

        except Exception as e:
            print(f"è­¦å‘Š: åŠ è½½ç¼“å­˜å¤±è´¥ {cache_path}: {e}")
            return None

    def _load_video_frames(self, video_path: str, max_frames=None):
        """åŠ è½½è§†é¢‘å¸§ï¼ˆä¸ longva_custom.py å®Œå…¨ä¸€è‡´ï¼‰"""
        from decord import VideoReader, cpu

        if max_frames is None:
            max_frames = self.nframe

        vr = VideoReader(video_path, ctx=cpu(0))
        total_frame_num = len(vr)

        # å‡åŒ€é‡‡æ ·ï¼ˆä¸ longva_custom.py å®Œå…¨ä¸€è‡´ï¼‰
        if total_frame_num <= max_frames:
            frame_idx = list(range(total_frame_num))
        else:
            uniform_sampled_frames = np.linspace(0, total_frame_num - 1, max_frames, dtype=int)
            frame_idx = uniform_sampled_frames.tolist()

        frames = vr.get_batch(frame_idx).asnumpy()
        return frames

    def use_custom_prompt(self, dataset):
        """Check if custom prompt should be used for a dataset."""
        return False

    def build_prompt(self, line, dataset=None):
        """Build prompt from dataset line."""
        import pandas as pd
        from ..smp import listinstr

        if isinstance(line, int):
            line = self.data.iloc[line]

        tgt_path = self.dump_image(line, dataset)

        question = line["question"]
        if "options" in line and not pd.isna(line["options"]):
            options = eval(line["options"]) if isinstance(line["options"], str) else line["options"]
            if isinstance(options, list):
                options_str = "\n".join([f"{chr(65+i)}. {opt}" for i, opt in enumerate(options)])
                question = f"{question}\n{options_str}"
            elif isinstance(options, dict):
                options_str = "\n".join([f"{k}. {v}" for k, v in options.items()])
                question = f"{question}\n{options_str}"

        msgs = [dict(type="image", value=p) for p in tgt_path]
        msgs.append(dict(type="text", value=question))
        return msgs

    def generate_inner(self, message, dataset=None):
        """Generate response with vision feature caching support."""
        # æå–è§†é¢‘è·¯å¾„å’Œæ–‡æœ¬
        video_path = None
        video_frames = None
        prompt_text = ""
        has_video = False

        for item in message:
            if item["type"] == "video":
                video_path = item["value"]
                has_video = True
            elif item["type"] == "image":
                # å›¾åƒæ¨¡å¼ï¼Œä½¿ç”¨åŸå§‹å®ç°
                return self._generate_image_mode(message)
            elif item["type"] == "text":
                if prompt_text:
                    prompt_text += "\n" + item["value"]
                else:
                    prompt_text = item["value"]

        if not has_video or video_path is None:
            # çº¯æ–‡æœ¬æ¨¡å¼
            return self._generate_text_mode(prompt_text)

        # è§†é¢‘æ¨¡å¼ï¼šå°è¯•ä½¿ç”¨ç¼“å­˜
        cached_vision_features = self._load_cached_vision_features(
            video_path, dataset, self.nframe
        )

        if cached_vision_features is not None:
            # ç¼“å­˜å‘½ä¸­ï¼šä½¿ç”¨ hook æ–¹å¼
            self.cache_hits += 1
            return self._generate_with_cached_features(
                video_path, cached_vision_features, prompt_text
            )
        else:
            # ç¼“å­˜æœªå‘½ä¸­ï¼šæ­£å¸¸æ¨ç†
            self.cache_misses += 1
            return self._generate_without_cache(video_path, prompt_text)

    def _generate_with_cached_features(self, video_path: str,
                                       cached_vision_features: torch.Tensor,
                                       prompt_text: str) -> str:
        """ä½¿ç”¨ç¼“å­˜çš„ vision features ç”Ÿæˆï¼ˆé€šè¿‡ hookï¼‰"""

        # è·å– vision_tower
        if hasattr(self.model, 'model') and hasattr(self.model.model, 'vision_tower'):
            vision_tower = self.model.model.vision_tower
        elif hasattr(self.model, 'vision_tower'):
            vision_tower = self.model.vision_tower
        else:
            # Fallbackï¼šæ— æ³• hookï¼Œä½¿ç”¨æ­£å¸¸æ¨ç†
            print("è­¦å‘Š: æ— æ³•è®¿é—® vision_towerï¼Œä½¿ç”¨æ­£å¸¸æ¨ç†")
            return self._generate_without_cache(video_path, prompt_text)

        # ä¿å­˜åŸå§‹ forward æ–¹æ³•
        original_forward = vision_tower.forward

        # å®šä¹‰ hookï¼šè¿”å›ç¼“å­˜çš„ç‰¹å¾
        def cached_forward(x):
            # æ³¨æ„ï¼šéœ€è¦æ·»åŠ  batch ç»´åº¦
            return cached_vision_features.unsqueeze(0)

        try:
            # æ›¿æ¢ forward æ–¹æ³•
            vision_tower.forward = cached_forward

            # ğŸš€ ä¼˜åŒ–ï¼šä¸åŠ è½½çœŸå®è§†é¢‘ï¼Œåˆ›å»º dummy tensor
            # åªéœ€è¦æ­£ç¡®çš„ shapeï¼Œå†…å®¹ä¸é‡è¦ï¼ˆå› ä¸º vision_tower ä¼šè¿”å›ç¼“å­˜ï¼‰
            # Shape: [nframe, 3, height, width]
            dummy_video_tensor = torch.zeros(
                (self.nframe, 3, 336, 336),
                dtype=torch.float16,
                device=self.model.device
            )

            # æ„å»º promptï¼ˆä¸ longva_custom.py:149 ä¸€è‡´ï¼‰
            prompt = f"<|im_start|>system\nYou are a helpful assistant.<|im_end|>\n<|im_start|>user\n<image>\n{prompt_text}<|im_end|>\n<|im_start|>assistant\n"

            input_ids = self.tokenizer_image_token(
                prompt, self.tokenizer, self.IMAGE_TOKEN_INDEX, return_tensors="pt"
            ).unsqueeze(0).to(self.model.device)

            # ç”Ÿæˆï¼ˆvision_tower ä¼šè¿”å›ç¼“å­˜çš„ç‰¹å¾ï¼Œmm_projector å’Œ LLM æ­£å¸¸è¿è¡Œï¼‰
            with torch.inference_mode():
                output_ids = self.model.generate(
                    input_ids,
                    images=[dummy_video_tensor],  # ä¼ å…¥ dummy tensorï¼ˆä¸ä¼šçœŸæ­£ä½¿ç”¨ï¼‰
                    modalities=["video"],
                    **self.gen_kwargs
                )

            outputs = self.tokenizer.batch_decode(output_ids, skip_special_tokens=True)[0].strip()
            return outputs

        finally:
            # æ¢å¤åŸå§‹ forward æ–¹æ³•
            vision_tower.forward = original_forward

    def _generate_without_cache(self, video_path: str, prompt_text: str) -> str:
        """æ­£å¸¸æ¨ç†ï¼ˆæ— ç¼“å­˜ï¼Œä¸ longva_custom.py å®Œå…¨ä¸€è‡´ï¼‰"""
        # åŠ è½½è§†é¢‘å¸§ï¼ˆä¸ longva_custom.py:125 ä¸€è‡´ï¼‰
        frames = self._load_video_frames(video_path)

        # é¢„å¤„ç†ï¼ˆä¸ longva_custom.py:145 ä¸€è‡´ï¼‰
        video_tensor = self.image_processor.preprocess(frames, return_tensors="pt")["pixel_values"]
        video_tensor = video_tensor.to(self.model.device, dtype=torch.float16)

        # æ„å»º promptï¼ˆä¸ longva_custom.py:149 ä¸€è‡´ï¼‰
        prompt = f"<|im_start|>system\nYou are a helpful assistant.<|im_end|>\n<|im_start|>user\n<image>\n{prompt_text}<|im_end|>\n<|im_start|>assistant\n"

        input_ids = self.tokenizer_image_token(
            prompt, self.tokenizer, self.IMAGE_TOKEN_INDEX, return_tensors="pt"
        ).unsqueeze(0).to(self.model.device)

        # ç”Ÿæˆï¼ˆä¸ longva_custom.py:156-162 ä¸€è‡´ï¼‰
        with torch.inference_mode():
            output_ids = self.model.generate(
                input_ids,
                images=[video_tensor],
                modalities=["video"],
                **self.gen_kwargs
            )

        outputs = self.tokenizer.batch_decode(output_ids, skip_special_tokens=True)[0].strip()
        return outputs

    def _generate_text_mode(self, prompt_text: str) -> str:
        """çº¯æ–‡æœ¬æ¨¡å¼ï¼ˆä¸ longva_custom.py:134-140 ä¸€è‡´ï¼‰"""
        prompt = f"<|im_start|>system\nYou are a helpful assistant.<|im_end|>\n<|im_start|>user\n{prompt_text}<|im_end|>\n<|im_start|>assistant\n"
        input_ids = self.tokenizer(prompt, return_tensors="pt").input_ids.to(self.model.device)
        with torch.inference_mode():
            output_ids = self.model.generate(input_ids, **self.gen_kwargs)
        outputs = self.tokenizer.batch_decode(output_ids, skip_special_tokens=True)[0].strip()
        return outputs

    def _generate_image_mode(self, message) -> str:
        """å›¾åƒæ¨¡å¼ï¼ˆä¸ longva_custom.py:167-196 ä¸€è‡´ï¼‰"""
        images = []
        prompt_text = ""

        for item in message:
            if item["type"] == "image":
                img = Image.open(item["value"]).convert("RGB")
                images.append(img)
            elif item["type"] == "text":
                if prompt_text:
                    prompt_text += "\n" + item["value"]
                else:
                    prompt_text = item["value"]

        # å¤„ç†å›¾åƒï¼ˆä¸ longva_custom.py:169-172 ä¸€è‡´ï¼‰
        images_tensor = self.process_images(images, self.image_processor, self.model.config)
        if isinstance(images_tensor, list):
            images_tensor = torch.stack(images_tensor, dim=0)
        images_tensor = images_tensor.to(self.model.device, dtype=torch.float16)

        # æ„å»º promptï¼ˆä¸ longva_custom.py:175-176 ä¸€è‡´ï¼‰
        image_tokens = "<image>" * len(images)
        prompt = f"<|im_start|>system\nYou are a helpful assistant.<|im_end|>\n<|im_start|>user\n{image_tokens}\n{prompt_text}<|im_end|>\n<|im_start|>assistant\n"

        input_ids = self.tokenizer_image_token(
            prompt, self.tokenizer, self.IMAGE_TOKEN_INDEX, return_tensors="pt"
        ).unsqueeze(0).to(self.model.device)

        # è·å–å›¾åƒå°ºå¯¸ï¼ˆä¸ longva_custom.py:183 ä¸€è‡´ï¼‰
        image_sizes = [img.size for img in images]

        # ç”Ÿæˆï¼ˆä¸ longva_custom.py:186-193 ä¸€è‡´ï¼‰
        with torch.inference_mode():
            output_ids = self.model.generate(
                input_ids,
                images=images_tensor,
                image_sizes=image_sizes,
                modalities=["image"] * len(images),
                **self.gen_kwargs
            )

        outputs = self.tokenizer.batch_decode(output_ids, skip_special_tokens=True)[0].strip()
        return outputs

    def __del__(self):
        """ææ„æ—¶è¾“å‡ºç¼“å­˜ç»Ÿè®¡"""
        if self.enable_cache and (self.cache_hits + self.cache_misses) > 0:
            total = self.cache_hits + self.cache_misses
            hit_rate = self.cache_hits / total * 100
            print(f"\n{'='*60}")
            print("Vision Cache Statistics")
            print(f"{'='*60}")
            print(f"Cache hits: {self.cache_hits}")
            print(f"Cache misses: {self.cache_misses}")
            print(f"Hit rate: {hit_rate:.1f}%")
            print(f"{'='*60}")
