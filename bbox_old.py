# -*- coding: utf-8 -*-
"""
LongVAâ€¯å¤šæ¨¡æ€æ¨¡å‹â€”â€”æ³¨æ„åŠ›å¯è§†åŒ–ç¤ºä¾‹è„šæœ¬ï¼ˆä¸­æ–‡è¯¦å°½æ³¨é‡Šç‰ˆï¼‰
----------------------------------------------------------
æœ¬è„šæœ¬æ¼”ç¤ºå¦‚ä½•ï¼š
1. åŠ è½½ LongVAâ€‘7Bâ€‘DPO å¤šæ¨¡æ€æ¨¡å‹ï¼ˆLLaVAâ€‘Qwen æ¶æ„ï¼‰ã€‚
2. å°†å¤šå¼ å›¾ç‰‡ï¼ˆå¯å¸¦æˆ–ä¸å¸¦ BBoxï¼‰ä¸æ–‡æœ¬é—®é¢˜æ‹¼æ¥æˆè¾“å…¥ã€‚
3. åœ¨è§£ç è¿‡ç¨‹ä¸­æ•è·æŸä¸ªâ€œneedleâ€è¯ï¼ˆä¾‹å¦‚ carï¼‰ç”Ÿæˆæ—¶çš„è‡ªæ³¨æ„åŠ›å¼ é‡ã€‚
4. è®¡ç®—å¹¶æ‰“å°æ¯ä¸€å±‚ã€æ¯ä¸ªæ³¨æ„åŠ›å¤´å¯¹æŒ‡å®š BBox å¯¹åº”è§†è§‰ token çš„å…³æ³¨åº¦ã€‚

âš ï¸ æ³¨æ„ï¼š
* è„šæœ¬é»˜è®¤ä½¿ç”¨ flashâ€‘attentionï¼›è‹¥ç¡¬ä»¶/é©±åŠ¨ä¸æ”¯æŒï¼Œå¯æ”¹æˆ "attn_mode='torch'"ã€‚
* è¯·ç¡®ä¿æ˜¾å­˜è¶³å¤Ÿï¼ˆè„šæœ¬å‰é¢è®¾ç½®äº† PYTORCH_CUDA_ALLOC_CONF ä»¥é™ä½ç¢ç‰‡åŒ–ï¼‰ã€‚
* éœ€è¦æå‰å‡†å¤‡å¥½ image1.JPG ä¸ target.JPG ä¸¤å¼ å›¾ç‰‡ï¼Œå¹¶æ ¹æ®éœ€è¦ä¿®æ”¹ BBox åæ ‡ã€‚
"""

import os
# é¿å… CUDA å†…å­˜ç¢ç‰‡è¿‡å¤§ï¼ŒæŒ‰ 128â€¯MB å¯¹é½åˆ†é…
os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "max_split_size_mb:128"

from longva.model.builder import load_pretrained_model  # LongVA æ¨¡å‹åŠ è½½å™¨
from transformers import AutoTokenizer                   # å¤‡ç”¨ï¼šè‹¥éœ€ç‹¬ç«‹è°ƒç”¨ tokenizer
from longva.mm_utils import tokenizer_image_token, process_images  # LongVA æä¾›çš„è¾…åŠ©å‡½æ•°
from longva.constants import IMAGE_TOKEN_INDEX           # ç‰¹æ®Š tokenï¼Œç”¨äºå ä½ <image>
from PIL import Image
import torch
import numpy as np
import matplotlib.pyplot as plt

# ä¸ºäº†ç»“æœå¯å¤ç°ï¼Œå›ºå®šéšæœºç§å­
torch.manual_seed(0)

# -------------------------
# 1. é…ç½®ä¸è¾“å…¥
# -------------------------
model_path = "lmms-lab/LongVA-7B-DPO"  # ğŸ¤– é¢„è®­ç»ƒæƒé‡åç§°ï¼ˆHF Hubï¼‰

# å›¾ç‰‡è·¯å¾„ä¸å¯¹åº” BBoxï¼ˆå·¦ä¸Š x,y, å³ä¸‹ x,yï¼‰ï¼›è‹¥æ—  BBox åˆ™å¡« None
images_and_bboxes = [
    ["image1.JPG", None],
    ["target.JPG", (1000, 2270, 2357, 2802)],
]

question = "What is the main object in the lower part of the second picture?"  # ç”¨æˆ·é—®é¢˜
needle = "car"  # æƒ³åœ¨ç”Ÿæˆæ–‡æœ¬ä¸­æ•è·çš„å…³é”®è¯

# -------------------------
# 2. åŠ è½½æ¨¡å‹ & é¢„å¤„ç†å›¾ç‰‡
# -------------------------
# tokenizer / model / image_processor å‡ç”± LongVA å°è£…è¿”å›
tokenizer, model, image_processor, _ = load_pretrained_model(
    model_path,
    None,                # é»˜è®¤ä½¿ç”¨å®˜æ–¹æƒé‡
    "llava_qwen",        # æ¨¡å‹æ¶æ„æ ‡è¯†
    device_map="auto"    # è‡ªåŠ¨æŠŠæƒé‡åˆ‡åˆ°å¤šå— GPUï¼ˆè‹¥å¯ç”¨ï¼‰
)

vision_tower = model.get_vision_tower()  # å–å‡ºè§†è§‰åˆ†æ”¯ï¼Œåç»­å¯ç”¨å…¶å±æ€§

# è¯»å–å¹¶è½¬æ¢å›¾ç‰‡ä¸º RGB
images = [Image.open(img_path).convert("RGB") for img_path, _ in images_and_bboxes]
# process_images ä¼šåš resize / normalizationï¼Œå¹¶è½¬æˆ (B,â€¯C,â€¯H,â€¯W) å¼ é‡
auto_dtype = torch.float16  # ä½¿ç”¨ fp16 èŠ‚çœæ˜¾å­˜
images_tensor = process_images(images, image_processor, model.config).to(model.device, dtype=auto_dtype)

# -------------------------
# 3. æ„é€ å¤šæ¨¡æ€ Prompt
# -------------------------
# LongVA çš„å¤šæ¨¡æ€æ ¼å¼ï¼šåœ¨æ–‡æœ¬ä¸­ç”¨ <image> å ä½ç¬¦æŒ‡ç¤ºå›¾åƒä½ç½®
prompt = (
    "<|im_start|>system\nYou are a helpful assistant.<|im_end|>\n"  # ç³»ç»ŸæŒ‡ä»¤
    f"<|im_start|>user\n" + "<image>\n" * len(images_and_bboxes) + f"{question}<|im_end|>\n"
    "<|im_start|>assistant\n"  # æ¨¡å‹å°†ä»è¿™é‡Œå¼€å§‹ç”Ÿæˆå›ç­”
)

# å°† prompt ç¼–ç ä¸º input_idsï¼Œå¹¶æŠŠ <image> æ›¿æ¢ä¸º IMAGE_TOKEN_INDEX
input_ids = tokenizer_image_token(prompt, tokenizer, IMAGE_TOKEN_INDEX, return_tensors="pt")
input_ids = input_ids.unsqueeze(0).to(model.device)  # (1,â€¯seq_len)

# -------------------------
# 4. å‡½æ•°ï¼šBBox â†’ è§†è§‰ token åŒºé—´æ˜ å°„
# -------------------------

def map_bbox_to_visual_token_ranges():
    """æ ¹æ®ç»™å®š BBox è®¡ç®—å…¶å¯¹åº”çš„è§†è§‰ token ç´¢å¼•åŒºé—´ã€‚

    å‡è®¾ Vision Tower æŠŠå›¾åƒåˆ†æˆ NÃ—N ä¸ª 336Ã—336 patchï¼Œå¹¶ä¸”æ¯ä¸ª patch
    å±•å¼€ä¸º 144 ä¸ª tokenï¼Œåˆ™å¯ä»¥ç”¨ block ç´¢å¼• * tokens_per_block è®¡ç®— token åŒºé—´ã€‚
    """
    block_size = 336           # Vision Tower é¢„è®¾çš„ patch å°ºå¯¸
    tokens_per_block = 144     # æ¯ä¸ª patch è¾“å‡ºçš„ token æ•°
    visual_token_start = 1     # æ³¨æ„ï¼š0 å·ä½ç½®æ˜¯ BOS

    # æ‰¾åˆ°å¸¦ BBox çš„é‚£å¼ å›¾ï¼ˆè¿™é‡Œåªå–ç¬¬ä¸€å¼ å¸¦ BBox çš„ï¼‰
    bbox_idx = next((i for i, (_, bbox) in enumerate(images_and_bboxes) if bbox), None)
    assert bbox_idx is not None, "å¿…é¡»æä¾›è‡³å°‘ä¸€ä¸ªå¸¦ BBox çš„å›¾ç‰‡"

    image_path, bbox = images_and_bboxes[bbox_idx]
    image = Image.open(image_path).convert("RGB")

    # ç”¨ image_processor å¾—åˆ°æ¨¡å‹å®é™…è¾“å…¥å°ºå¯¸ (processed_h, processed_w)
    raw_tensor = image_processor(image, return_tensors="pt")["pixel_values"]
    _, _, processed_h, processed_w = raw_tensor.shape

    # è®¡ç®— patch ç½‘æ ¼è¡Œåˆ—æ•°
    n_rows = processed_h // block_size
    n_cols = processed_w // block_size

    # å°†åŸå›¾åæ ‡ç¼©æ”¾åˆ° processed å°ºå¯¸
    x_scale, y_scale = processed_w / image.width, processed_h / image.height
    scaled_xmin, scaled_ymin = bbox[0] * x_scale, bbox[1] * y_scale
    scaled_xmax, scaled_ymax = bbox[2] * x_scale, bbox[3] * y_scale

    # éå†æ¯ä¸ª patchï¼Œè®¡ç®—ä¸ BBox çš„ IoUï¼ˆè¿™é‡Œåªç”¨é¢ç§¯å æ¯”ï¼‰
    selected_blocks = []        # æ»¡è¶³é˜ˆå€¼çš„ patch ç´¢å¼• (r,â€¯c)
    block_overlap_ratios = {}   # ä¿å­˜æ‰€æœ‰ patch çš„é‡å æ¯”ä¾‹ï¼Œæ–¹ä¾¿åå¤‡é€‰
    max_ratio = 0.0

    for r in range(n_rows):
        for c in range(n_cols):
            block_xmin, block_ymin = c * block_size, r * block_size
            block_xmax, block_ymax = (c + 1) * block_size, (r + 1) * block_size

            # äº¤é›†é¢ç§¯
            inter_w = max(0, min(block_xmax, scaled_xmax) - max(block_xmin, scaled_xmin))
            inter_h = max(0, min(block_ymax, scaled_ymax) - max(block_ymin, scaled_ymin))
            inter_area = inter_w * inter_h

            ratio = inter_area / (block_size * block_size)  # BBox å è¯¥ patch çš„æ¯”ä¾‹
            block_overlap_ratios[(r, c)] = ratio
            max_ratio = max(max_ratio, ratio)

            if ratio >= 0.5:  # å¦‚æœè¶…è¿‡ 50% é‡å å°±ç›´æ¥é€‰ä¸­
                selected_blocks.append((r, c))

    # è‹¥æ²¡æœ‰ patch æ»¡è¶³ 0.5 é˜ˆå€¼ï¼Œå°±æŒ‘é‡å ç‡æœ€é«˜çš„ä¸€æ‰¹
    if not selected_blocks:
        selected_blocks = [k for k, v in block_overlap_ratios.items() if np.isclose(v, max_ratio, atol=1e-6)]

    # è®¡ç®—è¿™äº› patch å¯¹åº”çš„ token ç´¢å¼•åŒºé—´
    selected_token_ranges = []
    for (r, c) in selected_blocks:
        block_idx = r * n_cols + c                 # flatten åçš„ patch åºå·
        token_start = visual_token_start + block_idx * tokens_per_block
        token_end = token_start + tokens_per_block
        selected_token_ranges.append((block_idx, token_start, token_end))

    print(f"âœ… ç²¾å‡†æ˜ å°„è§†è§‰ token èŒƒå›´ (Attention Index)ï¼š{selected_token_ranges}")
    return selected_token_ranges

# é¢„å…ˆè®¡ç®— BBox å¯¹åº”çš„ token åŒºé—´
selected_token_ranges = map_bbox_to_visual_token_ranges()

# -------------------------
# 5. é¦–æ¬¡å‰å‘ï¼šè·å– past_key_values ä»¥åŠ hidden_states é•¿åº¦
# -------------------------
max_new_tokens = 1000  # è§£ç æœ€å¤§é•¿åº¦

generated_ids = input_ids.clone()  # åˆå§‹åŒ–å·²ç”Ÿæˆåºåˆ— = è¾“å…¥åºåˆ—

eos_token_id = tokenizer.eos_token_id  # ç»ˆæ­¢ token
past = None                             # ç”¨äºå¢é‡è§£ç 
found_attention = None                  # å­˜å‚¨æ•è·åˆ°çš„æ³¨æ„åŠ›

auto_attn_mode = "flash"  # "flash" æˆ– "torch"

with torch.inference_mode():
    outputs = model(
        input_ids=input_ids,
        images=images_tensor,
        image_sizes=[img.size for img in images],
        modalities=["image"] * len(images),
        use_cache=True,
        output_attentions=False,   # é¦–æ¬¡ä¸éœ€è¦ attentions
        return_dict=True,
        attn_mode=auto_attn_mode,
        output_hidden_states=True  # æ–¹ä¾¿æˆ‘ä»¬çŸ¥é“åºåˆ—æ€»é•¿åº¦
    )
    past = outputs.past_key_values  # åç»­å¢é‡è§£ç è¦ç”¨

# ---------- æ‰“å°åºåˆ—é•¿åº¦ä¿¡æ¯ ----------
hidden_states = outputs.hidden_states[0]  # (1, seq_len, hidden_dim)
seq_total = hidden_states.shape[1]
print(f"\nğŸŸ¢ æ¨¡å‹å®é™…è¾“å…¥ embedding åºåˆ—é•¿åº¦ï¼ˆtext + vision token æ€»é•¿åº¦ï¼‰: {seq_total}")
print(f"ğŸŸ¢ hidden_states shape: {hidden_states.shape}")

# Vision Tower æ¯å¼ å›¾çš„ patch æ•°
num_patches_per_image = vision_tower.num_patches_per_side ** 2
num_image_tokens = num_patches_per_image * len(images)
print(f"ğŸŸ¢ å›¾ç‰‡å¯¹åº”çš„è§†è§‰ token æ•°é‡ï¼ˆå«æ‰€æœ‰å›¾ç‰‡ï¼‰: {num_image_tokens}")

text_token_len = seq_total - num_image_tokens  # è¿™é‡ŒåŒ…å« <image> å ä½ç¬¦
print(f"ğŸŸ¢ æ–‡æœ¬ token æ•°é‡: {text_token_len}")
print(f"ğŸŸ¢ å›¾ç‰‡è§†è§‰ token åœ¨ embedding ä¸­çš„èŒƒå›´: [{text_token_len}, {seq_total - 1}]")

# -------------------------
# 6. å¢é‡è§£ç  & æ•è·æ³¨æ„åŠ›
# -------------------------
temperature = 1.0

for step in range(max_new_tokens):
    with torch.inference_mode():
        current_input = generated_ids[:, -1:]  # ä»…è¾“å…¥æœ€åä¸€ä¸ª token
        outputs = model(
            input_ids=current_input,
            images=images_tensor,
            image_sizes=[img.size for img in images],
            modalities=["image"] * len(images),
            use_cache=True,
            past_key_values=past,
            output_attentions=True,   # éœ€è¦ attentionsï¼
            return_dict=True,
            attn_mode=auto_attn_mode
        )

        past = outputs.past_key_values  # æ›´æ–°ç¼“å­˜

        # é‡‡æ ·ä¸‹ä¸€ä¸ª token
        next_token_logits = outputs.logits[:, -1, :]
        probs = torch.softmax(next_token_logits / temperature, dim=-1)
        next_token_id = torch.multinomial(probs, num_samples=1)
        generated_ids = torch.cat([generated_ids, next_token_id], dim=-1)

        # è¿‡æ»¤æ‰ IMAGE_TOKEN_INDEXï¼Œåªçœ‹å¯è¯»æ–‡æœ¬
        filtered_ids = [tid.item() for tid in generated_ids[0] if tid.item() not in (-1, IMAGE_TOKEN_INDEX)]
        current_text = tokenizer.decode(filtered_ids, skip_special_tokens=True)

        # ä¸€æ—¦ç”Ÿæˆæ–‡æœ¬é‡Œå‡ºç° needleï¼Œå°±ä¿å­˜æ³¨æ„åŠ›
        if needle in current_text and found_attention is None:
            print(f"ğŸ¯ğŸ¯ğŸ¯ [NEEDLE FOUND at step {step}] ç”Ÿæˆä¸­å‘ç° needleï¼š'{needle}'")
            found_attention = [att.detach().cpu() for att in outputs.attentions]

        # ç”Ÿæˆåˆ° EOS å°±åœæ­¢
        if next_token_id[0] == eos_token_id:
            print(f"[Step {step}] é‡åˆ° EOSï¼Œç»“æŸ")
            break

    # æ‰‹åŠ¨é‡Šæ”¾æ˜¾å­˜
    del outputs
    torch.cuda.empty_cache()

# -------------------------
# 7. Attention åˆ†æ
# -------------------------
if found_attention is not None:
    print(f"âœ… Needle '{needle}' è¢«å‘ç°ï¼Œå¼€å§‹ Attention åˆ†æ")
    num_layers = len(found_attention)
    num_heads = found_attention[0][0].shape[0]

    for layer_idx, layer_attn in enumerate(found_attention):
        attn_tensor = layer_attn[0]  # shape = (num_heads, 1, seq_total)

        for head_idx in range(num_heads):
            head_attn = attn_tensor[head_idx, 0]  # å–å‡ºå•ä¸ª head çš„æ³¨æ„åŠ›å‘é‡
            head_attn[0] = 0.0  # å¿½ç•¥ BOS
            head_attn = torch.nan_to_num(head_attn.float(), nan=0.0)
            head_attn = torch.softmax(head_attn, dim=0)  # å½’ä¸€åŒ–åˆ°æ¦‚ç‡

            # ---- (A) è®¡ç®— BBox token çš„æ³¨æ„åŠ›æ€»å’Œ ----
            bbox_sum = sum(
                head_attn[start:end].sum().item()
                for _, start, end in selected_token_ranges
            )

            # ---- (B) è®¡ç®—é BBox token çš„æœ€å¤§æ³¨æ„åŠ› ----
            mask = torch.ones_like(head_attn, dtype=torch.bool)
            for _, start, end in selected_token_ranges:
                mask[start:end] = False
            non_bbox_max = head_attn[mask].max().item()

            # ---- (C) åˆ¤æ–­è¯¥ head æ˜¯å¦æ˜¾è‘—å…³æ³¨ BBox ----
            ratio = bbox_sum / (head_attn.sum().item() + 1e-8)
            if bbox_sum > non_bbox_max and ratio >= 0.1:
                print(
                    f"âœ… [L{layer_idx} H{head_idx}] BBox_sum: {bbox_sum:.4f} > max_other: {non_bbox_max:.4f} | å æ¯”: {ratio:.2%}"
                )

            # ---- (D) æ‰“å°è¯¥ head çš„ topâ€‘k æ³¨æ„åŠ› token ----
            topk_values, topk_indices = torch.topk(head_attn[1:], k=5)  # æ’é™¤ BOS
            print(
                f"Layer {layer_idx} Head {head_idx} topâ€‘5 attn tokens (index): "
                f"{(topk_indices + 1).tolist()} values: {topk_values.tolist()}"
            )
else:
    print(f"âŒ Needle '{needle}' æœªåŒ¹é…åˆ°ï¼Œè·³è¿‡ Attention åˆ†æ")