#!/usr/bin/env python3
import os
import argparse
import torch
from transformers import AutoModelForCausalLM
from longva.model.builder import load_pretrained_model
from longva.mm_utils import tokenizer_image_token, process_images
from longva.constants import IMAGE_TOKEN_INDEX


def get_transformer_layers(model):
    """
    è·å– Transformer æ¨¡å‹ä¸­çš„ encoder æˆ– decoder å±‚åˆ—è¡¨ï¼ˆå³ .layersï¼‰ã€‚
    è‡ªåŠ¨é€‚é…ä¸åŒæ¨¡å‹çš„å±‚çº§ç»“æ„ï¼Œå¦‚ Qwenã€LLaMAã€LongVA ç­‰ã€‚
    """
    if hasattr(model, "model") and hasattr(model.model, "layers"):
        return model.model.layers
    if hasattr(model, "model") and hasattr(model.model, "decoder") and hasattr(model.model.decoder, "layers"):
        return model.model.decoder.layers
    if hasattr(model, "llama") and hasattr(model.llama, "layers"):
        return model.llama.layers
    if hasattr(model, "transformer") and hasattr(model.transformer, "layers"):
        return model.transformer.layers
    raise AttributeError(f"æ— æ³•åœ¨æ¨¡å‹ {model.__class__.__name__} ä¸­æ‰¾åˆ° .layers å±æ€§")


def main():
    # 1. è§£æå‘½ä»¤è¡Œå‚æ•°ï¼šfusion Î±
    parser = argparse.ArgumentParser(description="Fuse LongVA & Qwen2 attention heads")
    parser.add_argument(
        "--alpha",
        type=float,
        default=0.5,
        help="Fusion ratio: 0 = pure LongVA, 1 = pure Qwen2 (0 <= Î± <= 1)"
    )
    parser.add_argument("--output_dir", type=str, default="/disk3/minami/huggingface/hub/models--LongVA-Merge")
    args = parser.parse_args()
    output_dir = args.output_dir

    # Clamp Î± åˆ° [0,1]
    ALPHA = max(0.0, min(1.0, args.alpha))
    # ALPHA = 1.0
    BETA = 1.0 - ALPHA
    print(f"[INFO] fusion Î± = {ALPHA:.2f}  (Î² = {BETA:.2f})")

    # 2. åŠ è½½ LongVA-7B-DPO æ¨¡å‹ï¼Œéƒ¨ç½²åˆ° GPU
    tokenizer, longva_model, image_processor, _ = load_pretrained_model(
        "lmms-lab/LongVA-7B-DPO",
        None,
        "llava_qwen",
        device_map="auto",
        torch_dtype=torch.float32,
        trust_remote_code=True
    )
    longva_model = longva_model.eval()
    longva_layers = get_transformer_layers(longva_model)

    # 3. åŠ è½½ Qwen2-7B-Instruct æ¨¡å‹ï¼Œä»…åŠ è½½åˆ° CPUï¼ˆä½œä¸ºå‚æ•°æºï¼‰
    qwen_model = AutoModelForCausalLM.from_pretrained(
        "Qwen/Qwen2-7B-Instruct",
        device_map="cpu",
        torch_dtype=torch.float32,
        trust_remote_code=True,
        low_cpu_mem_usage=True
    ).eval()
    qwen_layers = get_transformer_layers(qwen_model)

    # 4. æå–æ¨¡å‹ç»“æ„å‚æ•°
    cfg = longva_model.config
    H = cfg.num_attention_heads              # æ€»æ³¨æ„åŠ›å¤´æ•°
    KV = cfg.num_key_value_heads              # KV å…±äº«ç»„æ•°
    D = cfg.hidden_size                       # éšè—ç»´åº¦
    head_dim = D // H                         # æ¯ä¸ªå¤´çš„ç»´åº¦
    group_size = H // KV                      # æ¯ç»„åŒ…å«çš„å¤´æ•°

    # 5. æŒ‡å®šéœ€èåˆçš„æ³¨æ„åŠ›å¤´ï¼ˆæ¯å±‚çš„ head ä¸‹æ ‡åˆ—è¡¨ï¼‰
    # heads_to_merge = {
    #     0:  [3, 6, 7, 8, 10, 11, 12, 16, 17, 20, 23, 24, 25, 26, 27],
    #     1:  [0, 2, 3, 4, 10, 12, 15, 17, 19, 20, 21, 22, 25, 27],
    #     2:  [0, 8, 9, 10, 11, 27],
    #     3:  [7, 9, 10, 11, 13, 15],
    #     4:  [12, 14, 15, 16, 18, 19, 20, 23, 25],
    #     5:  [0, 1, 2, 3, 4, 7, 22, 23, 25, 26, 27],
    #     6:  [1, 3, 4, 5, 8, 10, 12, 14, 15, 16, 17, 19, 20, 21, 22, 24, 25, 26, 27],
    #     7:  [0, 1, 2, 3, 4, 6, 7, 8, 9, 10, 11, 12, 13, 14, 16, 17, 18, 20, 21, 24, 25, 26, 27],
    #     8:  [1, 8, 10, 12, 14, 15, 18, 20, 21],
    #     9:  [0, 1, 5, 6, 8, 11, 12, 13, 14, 16, 17, 18, 20, 21, 23, 26, 27],
    #     10: [1, 4, 5, 7, 8, 10, 11, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27],
    #     11: [1, 3, 4, 5, 6, 9, 10, 11, 14, 15, 16, 17, 18, 19, 20],
    #     12: [0],
    #     15: [8, 11, 22, 23, 26],
    #     18: [8, 25],
    #     19: [0, 4],
    #     20: [1, 2],
    #     21: [0, 11, 12, 15, 16, 18, 19, 20, 21, 23, 26],
    #     22: [0, 2, 9, 14, 18, 20],
    #     23: [7, 8, 10, 16, 17, 21, 22, 23, 24, 25, 26, 27],
    #     24: [0, 1, 2, 3, 4, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 20, 22, 27],
    #     25: [0, 1, 2, 3, 4, 5, 6, 14, 15, 16, 17, 18, 19, 20, 21, 22, 24, 26],
    #     26: [8, 10, 11, 12, 20, 21, 22, 23, 26, 27],
    #     27: [9, 25, 26]
    # }
    heads_to_merge = {
        0:  [3, 7, 10, 15, 16, 20],
        1:  [0, 2, 3, 4, 5, 10, 15, 17],
        3:  [7, 10],
        4:  [12, 14, 15, 18, 20],
        5:  [1, 22, 23, 25, 27],
        6:  [8, 14, 15, 16, 17, 19, 20, 21, 24, 25, 26, 27],
        7:  [0, 3, 7, 8, 9, 11, 12, 13, 14, 16, 20, 21],
        8:  [1, 14, 18],
        9:  [6, 12, 14, 16, 20],
        10: [5, 10, 15, 16, 17, 18, 22, 23, 25, 26, 27],
        11: [1, 3, 4, 5, 14, 15, 16, 17, 18, 19, 20],
        18: [7],
        19: [0, 4],
        21: [11, 15, 16, 18, 19, 20, 21],
        22: [2],
        23: [7, 10, 21, 22, 24, 26, 27],
        24: [0, 1, 2, 3, 4, 5, 6, 12, 16, 18, 20],
        25: [0, 1, 2, 3, 4, 5, 6, 14, 16, 17, 18, 19],
        26: [8, 12, 20, 22, 26],
        27: [7, 8, 9, 11]
    }

    # 6. éå†æ‰€æœ‰æŒ‡å®šå±‚å’Œå¤´ï¼Œé€ä¸ªå¤åˆ¶åˆå¹¶å‚æ•°ï¼ˆæŒ‰ Î±/Î² æ¯”ä¾‹çº¿æ€§æ’å€¼ï¼‰
    with torch.no_grad():
        for layer_idx, head_indices in heads_to_merge.items():
            attn_long = longva_layers[layer_idx].self_attn
            # å°† Qwen å¯¹åº”å±‚æ¬åˆ° GPU
            attn_qwen = qwen_layers[layer_idx].self_attn.to(attn_long.q_proj.weight.device)

            for h in head_indices:
                g = h // group_size
                q0, q1 = h * head_dim, (h + 1) * head_dim
                k0, k1 = g * head_dim, (g + 1) * head_dim

                # Q æŠ•å½±å‚æ•°
                attn_long.q_proj.weight.data[q0:q1] \
                    .mul_(BETA).add_(ALPHA * attn_qwen.q_proj.weight.data[q0:q1])
                if attn_long.q_proj.bias is not None:
                    attn_long.q_proj.bias.data[q0:q1] \
                        .mul_(BETA).add_(ALPHA * attn_qwen.q_proj.bias.data[q0:q1])

                # K æŠ•å½±å‚æ•°
                attn_long.k_proj.weight.data[k0:k1] \
                    .mul_(BETA).add_(ALPHA * attn_qwen.k_proj.weight.data[k0:k1])
                if attn_long.k_proj.bias is not None:
                    attn_long.k_proj.bias.data[k0:k1] \
                        .mul_(BETA).add_(ALPHA * attn_qwen.k_proj.bias.data[k0:k1])

                # V æŠ•å½±å‚æ•°
                attn_long.v_proj.weight.data[k0:k1] \
                    .mul_(BETA).add_(ALPHA * attn_qwen.v_proj.weight.data[k0:k1])
                if attn_long.v_proj.bias is not None:
                    attn_long.v_proj.bias.data[k0:k1] \
                        .mul_(BETA).add_(ALPHA * attn_qwen.v_proj.bias.data[k0:k1])

                # O æŠ•å½±å‚æ•°ï¼ˆåªå¯¹å½“å‰å¤´å¯¹åº”çš„åˆ—ï¼‰
                attn_long.o_proj.weight.data[:, q0:q1] \
                    .mul_(BETA).add_(ALPHA * attn_qwen.o_proj.weight.data[:, q0:q1])
                if attn_long.o_proj.bias is not None:
                    attn_long.o_proj.bias.data[q0:q1] \
                        .mul_(BETA).add_(ALPHA * attn_qwen.o_proj.bias.data[q0:q1])

                print(f"  âœ… Layer {layer_idx:2d}  Head {h:2d} merged (Î±={ALPHA:.2f})")

            # å›æ”¶ Qwen å±‚æ˜¾å­˜
            attn_qwen.to("cpu")
            torch.cuda.empty_cache()

    # 7. ä¿å­˜åˆå¹¶åçš„ LongVA æ¨¡å‹
    # output_dir = "/disk3/minami/huggingface/hub/models--LongVA-Merge"
    os.makedirs(output_dir, exist_ok=True)
    longva_model.save_pretrained(output_dir, safe_serialization=True)
    tokenizer.save_pretrained(output_dir)
    print(f"ğŸ‰ æ¨¡å‹åˆå¹¶å®Œæˆï¼Œå·²ä¿å­˜è‡³ï¼š{output_dir}")


if __name__ == "__main__":
    torch.manual_seed(0)
    main()