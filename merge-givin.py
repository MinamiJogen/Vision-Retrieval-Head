#!/usr/bin/env python3
import os
import argparse
import torch
from transformers import AutoModelForCausalLM
from longva.model.builder import load_pretrained_model
from longva.mm_utils import tokenizer_image_token, process_images
from longva.constants import IMAGE_TOKEN_INDEX


def get_transformer_layers(model):
    """
    è·å– Transformer æ¨¡å‹ä¸­çš„ encoder æˆ– decoder å±‚åˆ—è¡¨ï¼ˆå³ .layersï¼‰ã€‚
    è‡ªåŠ¨é€‚é…ä¸åŒæ¨¡å‹çš„å±‚çº§ç»“æ„ï¼Œå¦‚ Qwenã€LLaMAã€LongVA ç­‰ã€‚
    """
    if hasattr(model, "model") and hasattr(model.model, "layers"):
        return model.model.layers
    if hasattr(model, "model") and hasattr(model.model, "decoder") and hasattr(model.model.decoder, "layers"):
        return model.model.decoder.layers
    if hasattr(model, "llama") and hasattr(model.llama, "layers"):
        return model.llama.layers
    if hasattr(model, "transformer") and hasattr(model.transformer, "layers"):
        return model.transformer.layers
    raise AttributeError(f"æ— æ³•åœ¨æ¨¡å‹ {model.__class__.__name__} ä¸­æ‰¾åˆ° .layers å±æ€§")


def main():
    # 1. è§£æå‘½ä»¤è¡Œå‚æ•°ï¼šfusion Î±
    parser = argparse.ArgumentParser(description="Fuse LongVA & Qwen2 attention heads")
    parser.add_argument(
        "--alpha",
        type=float,
        default=0.8,
        help="Fusion ratio: 0 = pure LongVA, 1 = pure Qwen2 (0 <= Î± <= 1)"
    )
    parser.add_argument("--output_dir", type=str, default="/disk3/minami/huggingface/hub/models--LongVA-Merge")
    args = parser.parse_args()
    output_dir = args.output_dir

    # Clamp Î± åˆ° [0,1]
    ALPHA = max(0.0, min(1.0, args.alpha))
    # ALPHA = 1.0
    BETA = 1.0 - ALPHA
    print(f"[INFO] fusion Î± = {ALPHA:.2f}  (Î² = {BETA:.2f})")

    # 2. åŠ è½½ LongVA-7B æ¨¡å‹ï¼Œéƒ¨ç½²åˆ° GPU
    tokenizer, longva_model, image_processor, _ = load_pretrained_model(
        "lmms-lab/LongVA-7B",
        None,
        "llava_qwen",
        device_map="auto",
        torch_dtype=torch.float32,
        trust_remote_code=True
    )
    longva_model = longva_model.eval()
    longva_layers = get_transformer_layers(longva_model)

    # 3. åŠ è½½ Qwen2-7B-Instruct æ¨¡å‹ï¼Œä»…åŠ è½½åˆ° CPUï¼ˆä½œä¸ºå‚æ•°æºï¼‰
    qwen_model = AutoModelForCausalLM.from_pretrained(
        "Qwen/Qwen2-7B-Instruct",
        device_map="cpu",
        torch_dtype=torch.float32,
        trust_remote_code=True,
        low_cpu_mem_usage=True
    ).eval()
    qwen_layers = get_transformer_layers(qwen_model)

    # 4. æå–æ¨¡å‹ç»“æ„å‚æ•°
    cfg = longva_model.config
    H = cfg.num_attention_heads              # æ€»æ³¨æ„åŠ›å¤´æ•°
    KV = cfg.num_key_value_heads              # KV å…±äº«ç»„æ•°
    D = cfg.hidden_size                       # éšè—ç»´åº¦
    head_dim = D // H                         # æ¯ä¸ªå¤´çš„ç»´åº¦
    group_size = H // KV                      # æ¯ç»„åŒ…å«çš„å¤´æ•°

    # 5. æŒ‡å®šéœ€èåˆçš„æ³¨æ„åŠ›å¤´ï¼ˆæ¯å±‚çš„ head ä¸‹æ ‡åˆ—è¡¨ï¼‰
    heads_to_merge = {
        14: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27],
        16: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27],
        18: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27],
        19: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27],
        20: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27],
    }

    # 6. éå†æ‰€æœ‰æŒ‡å®šå±‚å’Œå¤´ï¼Œé€ä¸ªå¤åˆ¶åˆå¹¶å‚æ•°ï¼ˆæŒ‰ Î±/Î² æ¯”ä¾‹çº¿æ€§æ’å€¼ï¼‰
    with torch.no_grad():
        for layer_idx, head_indices in heads_to_merge.items():
            attn_long = longva_layers[layer_idx].self_attn
            attn_qwen = qwen_layers[layer_idx].self_attn.to(attn_long.q_proj.weight.device)

            merged_groups = set()
            for h in head_indices:
                g = h // group_size                  # å½“å‰å¤´æ‰€åœ¨çš„ KV ç»„
                q0, q1 = h * head_dim, (h + 1) * head_dim
                k0, k1 = g * head_dim, (g + 1) * head_dim

                # ---------------- Q ----------------
                attn_long.q_proj.weight.data[q0:q1] \
                    .mul_(BETA).add_(ALPHA * attn_qwen.q_proj.weight.data[q0:q1])
                if attn_long.q_proj.bias is not None:
                    attn_long.q_proj.bias.data[q0:q1] \
                        .mul_(BETA).add_(ALPHA * attn_qwen.q_proj.bias.data[q0:q1])

                # ----------- K / Vï¼ˆå»é‡å¤„ç†ï¼‰ ----------
                if g not in merged_groups:
                    for proj_long, proj_qwen in (
                        (attn_long.k_proj, attn_qwen.k_proj),
                        (attn_long.v_proj, attn_qwen.v_proj),
                    ):
                        proj_long.weight.data[k0:k1] \
                            .mul_(BETA).add_(ALPHA * proj_qwen.weight.data[k0:k1])
                        if proj_long.bias is not None:
                            proj_long.bias.data[k0:k1] \
                                .mul_(BETA).add_(ALPHA * proj_qwen.bias.data[k0:k1])
                    merged_groups.add(g)

                # ---------------- O ----------------
                attn_long.o_proj.weight.data[:, q0:q1] \
                    .mul_(BETA).add_(ALPHA * attn_qwen.o_proj.weight.data[:, q0:q1])
                if attn_long.o_proj.bias is not None:
                    attn_long.o_proj.bias.data[q0:q1] \
                        .mul_(BETA).add_(ALPHA * attn_qwen.o_proj.bias.data[q0:q1])

                print(f"  âœ… Layer {layer_idx:2d}  Head {h:2d} merged (Î±={ALPHA:.2f})")

            attn_qwen.to("cpu")
            torch.cuda.empty_cache()


    # 7. ä¿å­˜åˆå¹¶åçš„ LongVA æ¨¡å‹
    # output_dir = "/disk3/minami/huggingface/hub/models--LongVA-Merge"
    os.makedirs(output_dir, exist_ok=True)
    longva_model.save_pretrained(output_dir, safe_serialization=True)
    tokenizer.save_pretrained(output_dir)
    print(f"ğŸ‰ æ¨¡å‹åˆå¹¶å®Œæˆï¼Œå·²ä¿å­˜è‡³ï¼š{output_dir}")


if __name__ == "__main__":
    torch.manual_seed(0)
    main()