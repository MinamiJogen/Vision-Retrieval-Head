import os
import numpy as np
import torch
import matplotlib.pyplot as plt
import seaborn as sns
from tqdm import tqdm
from PIL import Image
from functools import partial

# A-OKVQA
from aokvqa.load_aokvqa import load_aokvqa, get_coco_path

# LongVAï¼ˆæ³¨æ„è¿™é‡Œè°ƒç”¨çš„æ˜¯åˆå¹¶åçš„æ¨¡å‹ï¼‰
from longva.model.builder import load_pretrained_model
from longva.mm_utils import tokenizer_image_token, process_images
from longva.constants import IMAGE_TOKEN_INDEX

attention_maps = {}

def hook_attention(module, input, output, layer_idx):
    """Hook to store attention weights."""
    if (
        isinstance(output, tuple)
        and len(output) > 1
        and isinstance(output[1], torch.Tensor)
    ):
        attention_maps[layer_idx] = output[1].detach().cpu()

def print_input_token_length(input_ids):
    """
    æ‰“å°æ¨¡å‹è¾“å…¥çš„ token é•¿åº¦ï¼Œä¸è¿›è¡Œ token è§£ç ã€‚
    """
    # input_ids çš„ shape ä¸º [batch, token_length]ï¼Œè¿™é‡Œ batch size ä¸º1
    token_length = input_ids.size(1)
    print(f"\nğŸ”¹ **Input token length: {token_length}**\n")
    
    # å¦‚æœéœ€è¦ï¼Œä¹Ÿå¯ä»¥æ‰“å° <image> token å‡ºç°çš„ä½ç½®ï¼ˆå¯é€‰ï¼‰
    input_ids_list = input_ids.squeeze(0).tolist()
    if IMAGE_TOKEN_INDEX in input_ids_list:
        image_token_pos = input_ids_list.index(IMAGE_TOKEN_INDEX)
        print(f"**<image> token found at position: {image_token_pos}**\n")
    else:
        print("**No <image> token found in input sequence!**\n")

def call_LongVA_with_attention(question, image_path, tokenizer, model, image_processor):
    global attention_maps
    attention_maps = {}  # æ¯æ¬¡è°ƒç”¨å‰é‡ç½® attention_maps

    # æ„é€  Prompt
    prompt = (
        "<|im_start|>system\nYou are a helpful assistant.<|im_end|>\n"
        "<|im_start|>user\n"
        f"{question}\n"
        "<image>\n"
        "<|im_end|>\n<|im_start|>assistant\n"
    )

    # (1) æ„é€ æ–‡æœ¬è¾“å…¥ input_idsï¼ˆint64 æ ¼å¼ï¼‰
    input_ids = tokenizer_image_token(
        prompt,
        tokenizer,
        IMAGE_TOKEN_INDEX,
        return_tensors="pt"
    ).unsqueeze(0).to(model.device)
    
    # è®°å½•å¹¶æ‰“å°è¾“å…¥ token çš„é•¿åº¦
    print_input_token_length(input_ids)

    # æ³¨æ„è¿™é‡Œè®¡ç®— split_index æ˜¯åŸºäº input_ids çš„ï¼Œ
    # ä¿è¯åç»­åˆ†ææ—¶ç”¨çš„ split ä¸ attention çš„ç»´åº¦å¯¹å¾—ä¸Š
    try:
        split_index = (input_ids[0] == IMAGE_TOKEN_INDEX).nonzero(as_tuple=True)[0][0].item()
    except IndexError:
        split_index = None

    # (2) å¤„ç†å›¾åƒï¼Œå¹¶è½¬ä¸º float16
    image = Image.open(image_path).convert("RGB")
    images_tensor = process_images([image], image_processor, model.config)
    images_tensor = images_tensor.to(model.device, dtype=torch.float16)

    # æ³¨å†Œ attention hook
    for i, layer in enumerate(model.model.layers):
        if hasattr(layer, "self_attn"):
            layer.self_attn.register_forward_hook(partial(hook_attention, layer_idx=i))

    # ç”Ÿæˆå‚æ•°é…ç½®
    gen_kwargs = {
        "do_sample": False,
        "num_beams": 1,
        "use_cache": True,
        "max_new_tokens": 512,
        "output_attentions": True
    }

    with torch.inference_mode():
        output_ids = model.generate(
            input_ids,
            images=images_tensor,
            image_sizes=[image.size],
            modalities=["image"],
            **gen_kwargs
        )

    output_text = tokenizer.batch_decode(output_ids, skip_special_tokens=True)[0].strip()
    # è¿”å› output_textã€attention_mapsã€split_index ä»¥åŠè¾“å…¥ token é•¿åº¦
    return output_text, attention_maps, split_index, input_ids.size(1)

# --------------------- ä¸»æµç¨‹ ---------------------
# ä¿®æ”¹è¿™é‡Œçš„ model_path ä¸ºåˆå¹¶åçš„æ¨¡å‹ç›®å½•
merged_model_path = "huggingface/hub/merged_longva"
print("ğŸ”„ Loading merged LongVA model, please wait...")

# 1) ä»¥ FP16 åŠ è½½åˆå¹¶åçš„æ¨¡å‹
tokenizer, model, image_processor, _ = load_pretrained_model(
    model_path=merged_model_path,
    model_base=None,
    model_name="longva_qwen",
    device_map="cuda:0",
    attn_implementation="eager",
    torch_dtype=torch.float16,   # ä»¥ float16 åŠ è½½æƒé‡
    load_8bit=False,
    load_4bit=False
)

# 2) å…³é—­é—ªå­˜æ³¨æ„åŠ›ï¼Œå¹¶å¼€å¯è¾“å‡ºæ³¨æ„åŠ›
if hasattr(model.config, "use_flash_attention"):
    model.config.use_flash_attention = False
model.config.output_attentions = True

model.eval()

# 3) å¼ºåˆ¶å°†æ‰€æœ‰æµ®ç‚¹å‹å‚æ•°è½¬æ¢ä¸º float16ï¼Œæ•´æ•°ç±»å‹ä¸å˜
for param in model.parameters():
    if param.is_floating_point():
        param.data = param.data.to(torch.float16)

for name, buf in model.named_buffers():
    # ä»…è½¬æ¢æµ®ç‚¹å‹ç¼“å†²åŒº
    if buf.is_floating_point():
        buf.data = buf.data.to(torch.float16)

# vision tower
if hasattr(model, "get_vision_tower"):
    vt = model.get_vision_tower()
    if vt is not None:
        # é€’å½’è½¬æ¢æ‰€æœ‰å‚æ•°
        for p in vt.parameters():
            if p.is_floating_point():
                p.data = p.data.to(torch.float16)
        for bn, bbuf in vt.named_buffers():
            if bbuf.is_floating_point():
                bbuf.data = bbuf.data.to(torch.float16)

# mm_projector
if hasattr(model.get_model(), "mm_projector"):
    mp = model.get_model().mm_projector
    for p in mp.parameters():
        if p.is_floating_point():
            p.data = p.data.to(torch.float16)
    for bn, bbuf in mp.named_buffers():
        if bbuf.is_floating_point():
            bbuf.data = bbuf.data.to(torch.float16)

print("Model loaded successfully, all floating buffers/params forced to float16, ints kept as int!")

# 4) å¤„ç† A-OKVQA æ•°æ®é›†
aokvqa_dir = "./aokvqa/datasets/aokvqa/"
coco_dir   = "./aokvqa/datasets/coco/"
train_dataset = load_aokvqa(aokvqa_dir, 'train')

# éšæœºæŠ½å– 5 ä¸ªæ ·æœ¬ï¼ˆè€Œéå‰ 5 ä¸ªä»»åŠ¡ï¼‰
num_samples = min(5, len(train_dataset))
sample_indices = np.random.choice(len(train_dataset), num_samples, replace=False)
output_dir = "Merged_attention_analysis"
os.makedirs(output_dir, exist_ok=True)

attention_results = {}
for i, idx in enumerate(tqdm(sample_indices, desc="Processing dataset", unit="sample")):
    dataset_example = train_dataset[idx]

    question_id = dataset_example['question_id']
    question = dataset_example['question']
    choices = dataset_example['choices']
    correct_choice = choices[dataset_example['correct_choice_idx']]
    image_path = get_coco_path('train', dataset_example['image_id'], coco_dir)

    # è·å–ç”Ÿæˆç»“æœï¼ŒåŒæ—¶è¿”å› attention_mapsã€split_index å’Œè¾“å…¥ token é•¿åº¦
    generated_text, attn_weights, split_index, input_token_length = call_LongVA_with_attention(
        question, image_path, tokenizer, model, image_processor
    )

    attention_results[question_id] = {
        "question": question,
        "choices": choices,
        "correct_choice": correct_choice,
        "generated_text": generated_text,
        "attention": attn_weights,
        "split_index": split_index,
        "input_token_length": input_token_length  # ä¿å­˜è¾“å…¥ token æ•°é‡
    }

    print(f"Processed {i+1}/{num_samples} - QID: {question_id}, Split Index: {split_index}")

np.savez_compressed(os.path.join(output_dir, "aokvqa.npz"), **attention_results)
print("Processing complete! Data saved.")
