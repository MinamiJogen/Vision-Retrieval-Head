import os
os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "max_split_size_mb:128"

from longva.model.builder import load_pretrained_model
from transformers import AutoTokenizer
from longva.mm_utils import tokenizer_image_token, process_images
from longva.constants import IMAGE_TOKEN_INDEX
from PIL import Image
import torch
import numpy as np
import matplotlib.pyplot as plt
import math
import argparse

torch.manual_seed(0)
model_path = "lmms-lab/LongVA-7B-DPO"

images_and_bboxes = [
    ["image1.JPG", None],
    ["target.JPG", (1000, 2270, 2357, 2802)],
]
question = "What is the main object in the lower part of the second picture?"
needle = "car"

# æ¨¡å‹åŠ è½½
tokenizer, model, image_processor, _ = load_pretrained_model(model_path, None, "llava_qwen", device_map="auto")
vision_tower = model.get_vision_tower()

images = [Image.open(img_path).convert("RGB") for img_path, _ in images_and_bboxes]
images_tensor = process_images(images, image_processor, model.config).to(model.device, dtype=torch.float16)

pre_prompt = "<|im_start|>system\nYou are a helpful assistant.<|im_end|>\n<|im_start|>user\n"
post_prompt = "<|im_end|>\n<|im_start|>assistant\n"

def vision_embedding(args):
    # æ¨¡å‹åŠ è½½ï¼šåŠ è½½ tokenizerã€modelã€image_processor ç­‰
    tokenizer, model, image_processor, context_len = load_pretrained_model(
        model_path, None, "llava_qwen", device_map="cuda:0"
    )
    # åˆ é™¤ä¸éœ€è¦çš„å±‚ï¼ˆåªä¿ç•™è§†è§‰ç¼–ç éƒ¨åˆ†ï¼‰
    del model.model.layers
    model.config.image_aspect_ratio = "pad"
    model.config.mm_patch_merge_type = "flat"

    # åŠ è½½å›¾ç‰‡ï¼ˆå¿½ç•¥ bbox ä¿¡æ¯ï¼‰
    images = [Image.open(img_path).convert("RGB") for img_path, _ in images_and_bboxes]
    print(f"åŠ è½½äº† {len(images)} å¼ å›¾ç‰‡")

    # é¢„å¤„ç†å›¾ç‰‡ï¼Œå¾—åˆ°æ¨¡å‹éœ€è¦çš„ tensor æ ¼å¼
    processed_images = process_images(images, image_processor, model.config).to(
        model.device, dtype=torch.float16
    )

    # é€šè¿‡æ¨¡å‹çš„ encode_images è·å–è§†è§‰ embeddingï¼ˆä¸ä½¿ç”¨ vision_towerï¼‰
    with torch.inference_mode():
        image_embeddings = model.encode_images(processed_images)
    # image_embeddings çš„å½¢çŠ¶é€šå¸¸ä¸º [B, L, F]ï¼Œå…¶ä¸­ B ä¸ºå›¾ç‰‡æ•°ï¼ŒL ä¸ºæ¯å¼ å›¾ç‰‡ç”Ÿæˆçš„ token æ•°ï¼ŒF ä¸ºç‰¹å¾ç»´åº¦

    # è‹¥è®¾ç½®äº† pooling_sizeï¼Œåˆ™å¯¹ç©ºé—´ç»´åº¦è¿›è¡Œæ± åŒ–
    if args.pooling_size != 0:
        B, L, F = image_embeddings.shape
        n = int(math.sqrt(L))
        image_embeddings_spatial = image_embeddings.view(B, n, n, F).permute(0, 3, 1, 2)  # [B, F, n, n]
        image_embeddings_spatial_pool = torch.nn.functional.avg_pool2d(
            image_embeddings_spatial, args.pooling_size, args.pooling_size
        )  # æ± åŒ–åå½¢çŠ¶ [B, F, new_n, new_n]
        # flatten å›åºåˆ—å½¢å¼ï¼Œå½¢çŠ¶å˜ä¸º [B, new_n*new_n, F]
        image_embeddings = image_embeddings_spatial_pool.flatten(2).transpose(1, 2).contiguous()

    # å°†æ¯å¼ å›¾ç‰‡çš„ embedding æ‹¼æ¥ä¸ºä¸€ä¸ªåºåˆ—ï¼ˆè¿™é‡Œå°†æ‰¹æ¬¡ç»´åº¦ merge åˆ°åºåˆ—ç»´åº¦ä¸­ï¼‰
    # ä¾‹å¦‚ä¸¤å¼ å›¾ç‰‡åˆ†åˆ«ç”Ÿæˆ [L1, F] å’Œ [L2, F]ï¼Œæ‹¼æ¥åå˜ä¸º [1, L1+L2, F]
    image_embeddings = image_embeddings.view(1, -1, image_embeddings.shape[-1])
    print(f"æœ€ç»ˆæ‹¼æ¥çš„è§†è§‰ embedding shape: {image_embeddings.shape}")

    # æ­¤å¤„ä»…æ‰“å°è§†è§‰ embedding æ‹¼æ¥åçš„å½¢çŠ¶ï¼Œä½ å¯ä»¥æ ¹æ®éœ€è¦åç»­ä½¿ç”¨ final_embeddings
    return image_embeddings

def safe_tokenize(tokenizer, text):
    """
    ä½¿ç”¨ tokenizer.encode() å¾—åˆ° token idï¼Œå¹¶å»é™¤ BOS tokenï¼ˆå¦‚æœå­˜åœ¨ï¼‰
    """
    tokenized = tokenizer.encode(text, return_tensors="pt")
    if tokenizer.bos_token is not None and tokenized.size(1) > 0 and tokenized[0, 0] == tokenizer.bos_token_id:
        tokenized = tokenized[:, 1:]
    return tokenized

def replace_double_newline_func(token_ids):
    """
    å°† token id 271 æ›¿æ¢ä¸ºä¸¤ä¸ª 198
    ä¾‹å¦‚ï¼šè¾“å…¥ tensor([[... 271, ...]]) æ›¿æ¢åé•¿åº¦ä¼šå¢åŠ 
    """
    # æ‰¾åˆ°æ‰€æœ‰ç­‰äº 271 çš„ä½ç½®ï¼ˆæ³¨æ„è¿™é‡Œå‡è®¾ token_ids çš„ shape ä¸º [1, seq_len]ï¼‰
    double_newline_loc = (token_ids == 271).nonzero()[:, 1]
    # è°ƒæ•´ä½ç½®ç´¢å¼•ï¼Œç¡®ä¿æ’å…¥ä¸ä¼šå‡ºé”™
    double_newline_loc += torch.arange(len(double_newline_loc))
    if len(double_newline_loc) > 0:
        for loc in double_newline_loc:
            # å°†å½“å‰ä½ç½® token ç”¨ä¸¤ä¸ª 198 æ›¿æ¢
            token_ids = torch.cat([
                token_ids[:, :loc],
                torch.tensor([[198, 198]], device=token_ids.device),
                token_ids[:, loc+1:]
            ], dim=1)
    return token_ids

def get_text_embedding(text, tokenizer, model, replace_double_newline=False, device=None):
    """
    æ¥æ”¶å­—ç¬¦ä¸² textï¼Œåˆ©ç”¨ tokenizer å’Œæ¨¡å‹çš„ embedding å±‚è·å–æ–‡æœ¬ embeddingã€‚
    
    å‚æ•°ï¼š
      - text: å¾…è½¬æ¢çš„å­—ç¬¦ä¸²
      - tokenizer: Hugging Face tokenizer å¯¹è±¡
      - model: åŒ…å« .model.embed_tokens æ–¹æ³•çš„æ¨¡å‹ï¼ˆå¦‚ Qwen2ã€LLaVA ç­‰ï¼‰
      - replace_double_newline: å¦‚æœä¸º Trueï¼Œåˆ™å°† token id 271 æ›¿æ¢æˆä¸¤ä¸ª 198
      - device: æŒ‡å®š deviceï¼Œä¸ä¼ æ—¶é»˜è®¤ä½¿ç”¨ model.device

    è¿”å›ï¼š
      - ä¸€ä¸ª tensorï¼Œå½¢çŠ¶ä¸º [1, seq_len, hidden_dim]ï¼Œæ•°æ®ç±»å‹ä¸º bfloat16
    """
    if device is None:
        device = model.device
    # è½¬ä¸º token id
    token_ids = safe_tokenize(tokenizer, text)
    if replace_double_newline:
        token_ids = replace_double_newline_func(token_ids)
    token_ids = token_ids.to(device)
    with torch.inference_mode():
        embeddings = model.model.embed_tokens(token_ids)
    return embeddings.to(torch.bfloat16)

parser = argparse.ArgumentParser()
parser.add_argument("--pooling_size", type=int, default=0, help="è®¾ç½®æ± åŒ–çª—å£å¤§å°ï¼Œ0 è¡¨ç¤ºä¸æ± åŒ–")
args = parser.parse_args()
vision_embeddings = vision_embedding(args)
pre_prompt_embeddings = get_text_embedding(pre_prompt, tokenizer, model, replace_double_newline=False)
post_prompt_embeddings = get_text_embedding(post_prompt, tokenizer, model, replace_double_newline=False)
question_embeddings = get_text_embedding(question, tokenizer, model, replace_double_newline=False)

input_emebds = torch.cat([pre_prompt_embeddings, vision_embeddings, question_embeddings, question_embeddings], dim=1)
total_seq_len = input_emebds.shape[1]
print(f"ğŸŸ¢ æ‹¼æ¥åçš„æ€» embedding åºåˆ—é•¿åº¦: {total_seq_len}")
print(f"    pre_prompt é•¿åº¦: {pre_prompt_embeddings.shape[1]}")
print(f"    è§†è§‰ embedding é•¿åº¦: {vision_embeddings.shape[1]}")
print(f"    question é•¿åº¦: {question_embeddings.shape[1]}")
print(f"    post_prompt é•¿åº¦: {post_prompt_embeddings.shape[1]}")

# æ„é€  position_idsï¼ˆshape: [1, total_seq_len]ï¼‰
position_ids = torch.arange(total_seq_len).unsqueeze(0).to(model.device)

# prompt = (
#     "<|im_start|>system\nYou are a helpful assistant.<|im_end|>\n"
#     f"<|im_start|>user\n" + f"{question}<|im_end|>\n"
#     "<|im_start|>assistant\n"
# )
#input_ids = tokenizer_image_token(prompt, tokenizer, IMAGE_TOKEN_INDEX, return_tensors="pt").unsqueeze(0).to(model.device)

# === æ¨ç†é˜¶æ®µ ===
import os
os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "max_split_size_mb:128"

from longva.model.builder import load_pretrained_model
from transformers import AutoTokenizer
from longva.mm_utils import tokenizer_image_token, process_images
from longva.constants import IMAGE_TOKEN_INDEX
from PIL import Image
import torch
import numpy as np
import matplotlib.pyplot as plt
import math
import argparse

torch.manual_seed(0)
model_path = "lmms-lab/LongVA-7B-DPO"

images_and_bboxes = [
    ["image1.JPG", None],
    ["target.JPG", (1000, 2270, 2357, 2802)],
]
question = "What is the main object in the lower part of the second picture?"
needle = "car"

# æ¨¡å‹åŠ è½½
tokenizer, model, image_processor, _ = load_pretrained_model(model_path, None, "llava_qwen", device_map="auto")

images = [Image.open(img_path).convert("RGB") for img_path, _ in images_and_bboxes]
images_tensor = process_images(images, image_processor, model.config).to(model.device, dtype=torch.float16)

pre_prompt = "<|im_start|>system\nYou are a helpful assistant.<|im_end|>\n<|im_start|>user\n"
post_prompt = "<|im_end|>\n<|im_start|>assistant\n"

def vision_embedding(args):
    # æ¨¡å‹åŠ è½½ï¼šåŠ è½½ tokenizerã€modelã€image_processor ç­‰
    tokenizer, model, image_processor, context_len = load_pretrained_model(
        model_path, None, "llava_qwen", device_map="cuda:0"
    )
    # åˆ é™¤ä¸éœ€è¦çš„å±‚ï¼ˆåªä¿ç•™è§†è§‰ç¼–ç éƒ¨åˆ†ï¼‰
    del model.model.layers
    model.config.image_aspect_ratio = "pad"
    model.config.mm_patch_merge_type = "flat"

    images = [Image.open(img_path).convert("RGB") for img_path, _ in images_and_bboxes]
    print(f"åŠ è½½äº† {len(images)} å¼ å›¾ç‰‡")

    processed_images = process_images(images, image_processor, model.config).to(
        model.device, dtype=torch.float16
    )

    with torch.inference_mode():
        image_embeddings = model.encode_images(processed_images)

    if args.pooling_size != 0:
        B, L, F = image_embeddings.shape
        n = int(math.sqrt(L))
        image_features_spatial = image_embeddings.view(B, n, n, F).permute(0, 3, 1, 2)
        image_features_spatial_pool = torch.nn.functional.avg_pool2d(
            image_features_spatial, args.pooling_size, args.pooling_size
        )
        image_embeddings = image_features_spatial_pool.flatten(2).transpose(1, 2).contiguous()

    image_embeddings = image_embeddings.view(1, -1, image_embeddings.shape[-1])
    print(f"æœ€ç»ˆæ‹¼æ¥çš„è§†è§‰ embedding shape: {image_embeddings.shape}")
    return image_embeddings

def safe_tokenize(tokenizer, text):
    tokenized = tokenizer.encode(text, return_tensors="pt")
    if tokenizer.bos_token is not None and tokenized.size(1) > 0 and tokenized[0, 0] == tokenizer.bos_token_id:
        tokenized = tokenized[:, 1:]
    return tokenized

def replace_double_newline_func(token_ids):
    double_newline_loc = (token_ids == 271).nonzero()[:, 1]
    double_newline_loc += torch.arange(len(double_newline_loc))
    if len(double_newline_loc) > 0:
        for loc in double_newline_loc:
            token_ids = torch.cat([
                token_ids[:, :loc],
                torch.tensor([[198, 198]], device=token_ids.device),
                token_ids[:, loc+1:]
            ], dim=1)
    return token_ids

def get_text_embedding(text, tokenizer, model, replace_double_newline=False, device=None):
    if device is None:
        device = model.device
    token_ids = safe_tokenize(tokenizer, text)
    if replace_double_newline:
        token_ids = replace_double_newline_func(token_ids)
    token_ids = token_ids.to(device)
    with torch.inference_mode():
        embeddings = model.model.embed_tokens(token_ids)
    return embeddings.to(torch.bfloat16)

parser = argparse.ArgumentParser()
parser.add_argument("--pooling_size", type=int, default=0, help="è®¾ç½®æ± åŒ–çª—å£å¤§å°ï¼Œ0 è¡¨ç¤ºä¸æ± åŒ–")
args = parser.parse_args()

##################################
# 1) DEBUG: æ‰“å°ä¸€äº›å…³é”® DTYPE
##################################
print("==> model.model.embed_tokens.weight.dtype:", model.model.embed_tokens.weight.dtype)
if hasattr(model.model, "layers") and len(model.model.layers) > 0:
    print("==> model.model.layers[0].self_attn.q_proj.weight.dtype:",
          model.model.layers[0].self_attn.q_proj.weight.dtype)

##################################
# 2) æ„å»ºembedding
##################################
vision_embeddings = vision_embedding(args)
pre_prompt_embeddings = get_text_embedding(pre_prompt, tokenizer, model, replace_double_newline=False)
post_prompt_embeddings = get_text_embedding(post_prompt, tokenizer, model, replace_double_newline=False)
question_embeddings = get_text_embedding(question, tokenizer, model, replace_double_newline=False)

input_emebds = torch.cat([pre_prompt_embeddings, vision_embeddings, question_embeddings, question_embeddings], dim=1)
print(f"ğŸŸ¢ åŸå§‹ input_emebds.dtype = {input_emebds.dtype}")
print(f"ğŸŸ¢ æ‹¼æ¥åçš„æ€»é•¿åº¦: {input_emebds.shape[1]}")

##################################
# 3) å¯é€‰ï¼šå°†embeddingè½¬åˆ°æ¨¡å‹æƒé‡ç›¸åŒçš„ dtype
##################################
weight_dtype = model.model.embed_tokens.weight.dtype
if input_emebds.dtype != weight_dtype:
    print(f"==> converting input_emebds from {input_emebds.dtype} to {weight_dtype} ...")
    input_emebds = input_emebds.to(weight_dtype)
print(f"ğŸŸ¢ è½¬æ¢å input_emebds.dtype = {input_emebds.dtype}")

# æ„é€  position_idsï¼ˆshape: [1, total_seq_len]ï¼‰
position_ids = torch.arange(input_emebds.shape[1]).unsqueeze(0).to(model.device)

def eval_forward(model, input_embeds, tokenizer, max_new_tokens=50, temperature=1.0, device=None):
    """
    æ¥å—æ‹¼æ¥å¥½çš„ embedding (promptçš„embedding)ï¼Œå¹¶ç”Ÿæˆæ–‡æœ¬è¾“å‡ºã€‚

    å‚æ•°ï¼š
        model: Huggingface transformers æ¨¡å‹
        input_embeds: torch.Tensor, [1, seq_len, hidden_dim]
        tokenizer: å¯¹åº”çš„ tokenizer
        max_new_tokens: ç”Ÿæˆæœ€å¤§é•¿åº¦
        temperature: æ§åˆ¶éšæœºæ€§ç¨‹åº¦çš„è¶…å‚
        device: torch.device

    è¿”å›ï¼š
        generated_text: æ¨¡å‹ç”Ÿæˆçš„æ–‡æœ¬ï¼ˆå­—ç¬¦ä¸²ï¼‰
    """
    if device is None:
        device = input_embeds.device

    generated_embeds = input_embeds.clone()
    past = None
    generated_ids = []

    with torch.inference_mode():
        # é¦–å…ˆforwardä¸€æ¬¡ï¼Œå¾—åˆ°åˆå§‹past_key_values
        outputs = model(
            inputs_embeds=generated_embeds,
            use_cache=True,
            output_attentions=False,
            return_dict=True
        )
        past = outputs.past_key_values

        for _ in range(max_new_tokens):
            logits = outputs.logits[:, -1, :] / temperature
            next_token_id = torch.argmax(logits, dim=-1, keepdim=True)
            generated_ids.append(next_token_id.item())

            # é‡åˆ°EOSç»“æŸ
            if next_token_id.item() == tokenizer.eos_token_id:
                break

            next_token_embed = model.model.embed_tokens(next_token_id).to(input_embeds.dtype)

            outputs = model(
                inputs_embeds=next_token_embed,
                past_key_values=past,
                use_cache=True,
                return_dict=True
            )
            past = outputs.past_key_values

    generated_text = tokenizer.decode(generated_ids, skip_special_tokens=True)
    return generated_text

generated_text = eval_forward(
    model=model,
    input_embeds=input_emebds,
    tokenizer=tokenizer,
    max_new_tokens=50,
    temperature=0.9
)

print("æ¨¡å‹ç”Ÿæˆçš„æ–‡æœ¬æ˜¯ï¼š", generated_text)
##################################
# 4) è¿›å…¥æ¨ç†é˜¶æ®µ
##################################
# max_new_tokens = 1000
# generated_embeds = input_emebds.clone()  # [1, prompt_len, hidden_dim]
# generated_ids = torch.tensor([], dtype=torch.long, device=model.device)
# eos_token_id = tokenizer.eos_token_id
# past = None
# temperature = 1
# found_attention = None

# print("\n==> Starting forward with inputs_embeds:")
# print("    generated_embeds dtype:", generated_embeds.dtype)
# print("    images_tensor dtype:", images_tensor.dtype)

# with torch.inference_mode():
#     outputs = model(
#         inputs_embeds=generated_embeds,
#         images=images_tensor,
#         image_sizes=[img.size for img in images],
#         modalities=["image"] * len(images),
#         use_cache=True,
#         output_attentions=False,
#         return_dict=True,
#         attn_mode="flash",
#         output_hidden_states=True
#     )
#     past = outputs.past_key_values

# for step in range(max_new_tokens):
#     with torch.inference_mode():
#         next_token_logits = outputs.logits[:, -1, :]
#         probs = torch.softmax(next_token_logits / temperature, dim=-1)
#         next_token_id = torch.multinomial(probs, num_samples=1)

#         if generated_ids.numel() > 0:
#             generated_ids = torch.cat([generated_ids, next_token_id], dim=1)
#         else:
#             generated_ids = next_token_id

#         # ç”Ÿæˆ new token embed
#         next_token_embed = model.model.embed_tokens(next_token_id)
#         # debug print
#         if step < 3:  # å‰3æ­¥çœ‹ä¸€ä¸‹
#             print(f"==> step {step}: next_token_embed.dtype = {next_token_embed.dtype}")
#         # è½¬æˆå’Œæƒé‡åŒ¹é…
#         next_token_embed = next_token_embed.to(weight_dtype)

#         generated_embeds = torch.cat([generated_embeds, next_token_embed], dim=1)

#         outputs = model(
#             inputs_embeds=next_token_embed,
#             images=images_tensor,
#             image_sizes=[img.size for img in images],
#             modalities=["image"] * len(images),
#             use_cache=True,
#             past_key_values=past,
#             output_attentions=True,
#             return_dict=True,
#             attn_mode="flash"
#         )
#         past = outputs.past_key_values

#         if next_token_id[0, 0] == eos_token_id:
#             print(f"[Step {step}] Encountered EOS, stopping generation.")
#             break

# final_output = tokenizer.decode(generated_ids.squeeze().tolist(), skip_special_tokens=True)
# print("\næœ€ç»ˆç”Ÿæˆæ–‡æœ¬ï¼š")
# print(final_output)


# # === è§£æ attention å¹¶æ‰“å°å…³æ³¨åº¦ ===
# if found_attention is not None:
#     print(f"âœ… Needle '{needle}' è¢«å‘ç°ï¼Œå¼€å§‹ Attention åˆ†æ")
    
#     # è§†è§‰ token åŒºé—´ï¼ˆåŸºäºä¹‹å‰è®¡ç®—ï¼‰
#     vision_start = text_token_len
#     vision_end = total_seq_len - 1
#     print(f"ğŸ¯ è§†è§‰ token èŒƒå›´: [{vision_start}, {vision_end}]")

#     for layer_idx, layer_attn in enumerate(found_attention):
#         print(f"\nğŸ§  Layer {layer_idx} attention shape: {layer_attn.shape}")
#         batch_size, num_heads, query_len, seq_len = layer_attn.shape
#         assert batch_size == 1 and query_len == 1, "batch=1ï¼Œquery=1æ‰ç¬¦åˆç”Ÿæˆåœºæ™¯"

#         for head_idx in range(num_heads):
#             attn_scores = layer_attn[0, head_idx, 0, :]  # shape: (seq_len,)
#             attn_scores_no_bos = attn_scores.clone()
#             attn_scores_no_bos[0] = 0.0  # å»é™¤BOSå½±å“

#             total_attn = attn_scores_no_bos.sum().item() + 1e-8
#             max_score, max_idx = torch.max(attn_scores_no_bos, dim=0)
#             ratio = max_score.item() / total_attn

#             # âœ… åªå…³å¿ƒè§†è§‰åŒºåŸŸ token
#             if vision_start <= max_idx.item() <= vision_end:
#                 print(f"   ğŸ” Head {head_idx}: most attended token idx = {max_idx.item()} "
#                       f"(âœ… è§†è§‰åŒºåŸŸ), score = {max_score.item():.6f}, "
#                       f"å æ¯” = {ratio * 100:.2f}%")

    # num_layers = len(found_attention)
    # num_heads = found_attention[0][0].shape[0]

#     for layer_idx, layer_attn in enumerate(found_attention):
#         attn_tensor = layer_attn[0]  # (num_heads, 1, total_tokens)
#         for head_idx in range(num_heads):
#             head_attn = attn_tensor[head_idx, 0]
#             head_attn[0] = 0.0  # å»æ‰ BOS
#             head_attn = torch.nan_to_num(head_attn.float(), nan=0.0)
#             head_attn = torch.softmax(head_attn, dim=0)

#             # è®¡ç®— BBox è§†è§‰ token çš„ attention æ€»å’Œ
#             b_score = sum([head_attn[start:end].sum().item() for _, start, end in selected_token_ranges])

#             # é BBox åŒºåŸŸæœ€å¤§ token attention
#             mask = torch.ones_like(head_attn, dtype=torch.bool)
#             for _, start, end in selected_token_ranges:
#                 mask[start:end] = False
#             non_bbox_max = head_attn[mask].max().item()

#             b_ratio = b_score / (head_attn.sum().item() + 1e-8)
#             if b_score > non_bbox_max and b_ratio >= 0.1:
#                 print(f"âœ… [L{layer_idx} H{head_idx}] BBox_sum: {b_score:.4f} > max_other: {non_bbox_max:.4f} | å æ¯”: {b_ratio:.2f}")

#             # ğŸ”¥ æ‰“å° top-k å…³æ³¨ token
#             topk_values, topk_indices = torch.topk(head_attn[1:], k=5)  # æ’é™¤BOS
#             print(f"Layer {layer_idx} Head {head_idx} top-5 attn tokens (attention idx): {topk_indices + 1} values: {topk_values.tolist()}")

# else:
#     print(f"âŒ Needle '{needle}' æœªåŒ¹é…åˆ°ï¼Œè·³è¿‡ Attention åˆ†æ")
