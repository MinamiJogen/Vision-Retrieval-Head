import os
os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "max_split_size_mb:128"

from longva.model.builder import load_pretrained_model
from transformers import AutoTokenizer
from longva.mm_utils import tokenizer_image_token, process_images
from longva.constants import IMAGE_TOKEN_INDEX
from PIL import Image
import torch
import numpy as np
import matplotlib.pyplot as plt

torch.manual_seed(0)
model_path = "lmms-lab/LongVA-7B-DPO"

images_and_bboxes = [
    ["image1.JPG", None],
    ["image3.JPG", None],
    ["image2.JPG", (1000, 2270, 2357, 2802)],
    ["target.JPG", None],
]
question = "What animal is lying on the grass?"
needle = "cat"

# === æ¨¡å‹åŠ è½½ ===
tokenizer, model, image_processor, _ = load_pretrained_model(model_path, None, "llava_qwen", device_map="cuda")
vision_tower = model.get_vision_tower()

images = [Image.open(img_path).convert("RGB") for img_path, _ in images_and_bboxes]
images_tensor = process_images(images, image_processor, model.config).to(model.device, dtype=torch.float16)

prompt = (
    "<|im_start|>system\nYou are a helpful assistant.<|im_end|>\n"
    f"<|im_start|>user\n" + "<image>\n" + f"{question}<|im_end|>\n"
    "<|im_start|>assistant\n"
)

input_ids = tokenizer_image_token(prompt, tokenizer, IMAGE_TOKEN_INDEX, return_tensors="pt").unsqueeze(0).to(model.device)

# === æ¨ç†é˜¶æ®µ ===
max_new_tokens = 1000
generated_ids = input_ids.clone()
eos_token_id = tokenizer.eos_token_id
past = None
temperature = 1
found_attention = None

with torch.inference_mode():
    outputs = model(
        input_ids=input_ids,
        images=images_tensor,
        image_sizes=[img.size for img in images],
        modalities=["image"] * len(images),
        use_cache=True,
        output_attentions=False,
        return_dict=True,
        attn_mode="flash",
        output_hidden_states=True
    )
    past = outputs.past_key_values

# âœ… æå– hidden_states å¹¶è®¡ç®— token åˆ†å¸ƒ
hidden_states = outputs.hidden_states[0]  # (batch, seq_len, hidden_dim)
total_seq_len = hidden_states.shape[1]
print(f"\nğŸŸ¢ æ¨¡å‹å®é™…è¾“å…¥ embedding åºåˆ—é•¿åº¦ï¼ˆtext + vision token æ€»é•¿åº¦ï¼‰: {total_seq_len}")
print(f"ğŸŸ¢ hidden_states shape: {hidden_states.shape}")

# num_image_patches = model.get_vision_tower().num_patches_per_side ** 2
# num_image_tokens = num_image_patches * len(images)
# text_token_len = total_seq_len - num_image_tokens
# print(f"ğŸŸ¢ å›¾ç‰‡å¯¹åº”çš„è§†è§‰ token æ•°é‡ï¼ˆå«æ‰€æœ‰å›¾ç‰‡ï¼‰: {num_image_tokens}")
# print(f"ğŸŸ¢ æ–‡æœ¬ token æ•°é‡: {text_token_len}")
# print(f"ğŸŸ¢ å›¾ç‰‡è§†è§‰ token åœ¨ embedding ä¸­çš„èŒƒå›´: [{text_token_len}, {total_seq_len - 1}]")

# # âœ… è·å–å¸¦ bbox å›¾ç‰‡çš„è§†è§‰ token èŒƒå›´
# def compute_selected_token_ranges_from_output(hidden_states, bbox_idx, num_images, num_patches_per_image):
#     total_seq_len = hidden_states.shape[1]
#     text_token_len = total_seq_len - num_patches_per_image * num_images
#     token_start = text_token_len + bbox_idx * num_patches_per_image
#     token_end = token_start + num_patches_per_image
#     return [(bbox_idx, token_start, token_end)], text_token_len, total_seq_len

# # ğŸ” æ‰¾åˆ°å¸¦ bbox çš„å›¾ç‰‡ç´¢å¼•
# bbox_idx = next((i for i, (_, bbox) in enumerate(images_and_bboxes) if bbox is not None), None)
# assert bbox_idx is not None, "å¿…é¡»æä¾›å¸¦æœ‰ bbox çš„å›¾ç‰‡"

# selected_token_ranges, text_token_len, total_seq_len = compute_selected_token_ranges_from_output(
#     hidden_states, bbox_idx, len(images), num_image_patches
# )
# print(f"âœ… ä½¿ç”¨å¸¦æœ‰ bbox çš„å›¾ç‰‡çš„è§†è§‰ token èŒƒå›´ (Attention Index)ï¼š{selected_token_ranges}")

num_patches_per_grid = vision_tower.num_patches_per_side ** 2

# === ç²¾ç¡®è®¡ç®—æ¯å¼ å›¾ç‰‡å¯¹åº”çš„è§†è§‰ token æ•°é‡ ===
grids_per_image_list = []
for img in images:
    patches_tensor = process_images([img], image_processor, model.config)
    grids_per_image_list.append(patches_tensor.shape[0])

tokens_per_image_list = [g * num_patches_per_grid for g in grids_per_image_list]

# === è®¡ç®—æ–‡æœ¬ token é•¿åº¦ ===
num_image_tokens = sum(tokens_per_image_list)
text_token_len = total_seq_len - num_image_tokens

print(f"ğŸŸ¢ å›¾ç‰‡å¯¹åº”çš„è§†è§‰ token æ€»æ•°é‡ï¼ˆæ‰€æœ‰å›¾ç‰‡ï¼‰: {num_image_tokens}")
print(f"ğŸŸ¢ æ–‡æœ¬ token æ•°é‡: {text_token_len}")
print(f"ğŸŸ¢ å›¾ç‰‡è§†è§‰ token åœ¨ embedding ä¸­çš„èŒƒå›´: [{text_token_len}, {total_seq_len - 1}]")

# === è®¡ç®—å¸¦ bbox å›¾ç‰‡çš„è§†è§‰ token èŒƒå›´ ===
bbox_idx = next((i for i, (_, bbox) in enumerate(images_and_bboxes) if bbox is not None), None)
assert bbox_idx is not None, "å¿…é¡»æä¾›å¸¦æœ‰ bbox çš„å›¾ç‰‡"

token_cursor = text_token_len
for idx in range(bbox_idx):
    token_cursor += tokens_per_image_list[idx]

token_start = token_cursor
token_end = token_start + tokens_per_image_list[bbox_idx]

selected_token_ranges = [(bbox_idx, token_start, token_end)]

print(f"âœ… å¸¦æœ‰ bbox çš„å›¾ç‰‡è§†è§‰ token ç²¾ç¡®èŒƒå›´ (Attention Index)ï¼š{selected_token_ranges}")


# === ç”Ÿæˆæ–° token ===
for step in range(max_new_tokens):
    with torch.inference_mode():
        current_input = generated_ids[:, -1:]
        outputs = model(
            input_ids=current_input,
            images=images_tensor,
            image_sizes=[img.size for img in images],
            modalities=["image"] * len(images),
            use_cache=True,
            past_key_values=past,
            output_attentions=True,
            return_dict=True,
            attn_mode="flash"
        )
        past = outputs.past_key_values
        next_token_logits = outputs.logits[:, -1, :]
        probs = torch.softmax(next_token_logits / temperature, dim=-1)
        next_token_id = torch.multinomial(probs, num_samples=1)
        generated_ids = torch.cat([generated_ids, next_token_id], dim=-1)

        filtered_ids = [tid.item() for tid in generated_ids[0] if tid.item() >= 0 and tid.item() != IMAGE_TOKEN_INDEX]
        current_text = tokenizer.decode(filtered_ids, skip_special_tokens=True)

        if needle in current_text and found_attention is None:
            print(f"ğŸ¯ğŸ¯ğŸ¯ [NEEDLE FOUND at step {step}] ç”Ÿæˆä¸­å‘ç° needleï¼š'{needle}'")
            found_attention = [att.detach().cpu() for att in outputs.attentions]

        if next_token_id[0] == eos_token_id:
            print(f"[Step {step}] é‡åˆ° EOSï¼Œç»“æŸ")
            break
    del outputs
    torch.cuda.empty_cache()

# === åå¤„ç† ===
clean_generated_ids = [
    token_id.item() for token_id in generated_ids[0]
    if token_id.item() >= 0 and token_id.item() != IMAGE_TOKEN_INDEX
]
final_output = tokenizer.decode(clean_generated_ids, skip_special_tokens=True)

print("\næœ€ç»ˆç”Ÿæˆæ–‡æœ¬ï¼š")
print(final_output)

# === è§£æ attention å¹¶æ‰“å°å…³æ³¨åº¦ï¼ˆæ¯å±‚æ‰“å°æœ€å…³æ³¨ç›®æ ‡å›¾ç‰‡çš„ headï¼‰===
if found_attention is not None:
    print(f"\nâœ… Needle '{needle}' è¢«å‘ç°ï¼Œå¼€å§‹ Attention åˆ†æ")

    # === é€‰ä¸­ token ç´¢å¼• ===
    selected_token_indices = []
    for _, start, end in selected_token_ranges:
        selected_token_indices.extend(range(start, end))
    selected_token_indices = torch.tensor(selected_token_indices, device="cpu")

    print(f"ğŸ¯ æ‰€é€‰å›¾ç‰‡è§†è§‰ token åŒºé—´: {selected_token_ranges}")
    print(f"ğŸ¯ æ‰€é€‰ token æ€»æ•°: {len(selected_token_indices)}")
    print(f"ğŸ¯ found_attention å±‚æ•°: {len(found_attention)}\n")

    attention_ratio_threshold = 0.08  # å¯è°ƒï¼šè¶…è¿‡è¿™ä¸ªå€¼æ‰è®¤ä¸ºæ˜¯é«˜å…³æ³¨

    for layer_idx, layer_attn in enumerate(found_attention):
        attn_tensor = layer_attn[0]  # shape: (num_heads, 1, seq_len)
        num_heads = attn_tensor.shape[0]

        for head_idx in range(num_heads):
            head_attn = attn_tensor[head_idx, 0].clone()
            head_attn[0] = 0.0  # å»æ‰ BOS
            head_attn = torch.nan_to_num(head_attn.float(), nan=0.0)
            # ä¸å† softmaxï¼Œå› ä¸º flash attention å·²æ˜¯ normalized åˆ†å¸ƒ

            selected_sum = head_attn[selected_token_indices].sum().item()
            total_sum = head_attn.sum().item() + 1e-8
            ratio = selected_sum / total_sum

            if ratio >= attention_ratio_threshold:
                print(f"ğŸ”¥ Layer {layer_idx:02d} Head {head_idx:02d}: {ratio:.4f}")

    # âœ… æ‰“å°æ¯å±‚ attention çš„ shapeï¼ˆç¡®è®¤ç»“æ„ï¼‰
    shape_info = [a.shape for a in found_attention]
    print(f"\nğŸ“ æå–åˆ°çš„ attention shapeï¼ˆæ¯å±‚ï¼‰:")
    for i, shape in enumerate(shape_info):
        print(f"  Layer {i:02d}: {shape}")
else:
    print(f"\nâŒ Needle '{needle}' æœªåŒ¹é…åˆ°ï¼Œè·³è¿‡ Attention åˆ†æ")