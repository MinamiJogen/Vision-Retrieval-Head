=========================================
å¼€å§‹æµ‹è¯• Qwen2.5-VL-7B-Instruct - Thu 04 Sep 2025 03:04:23 AM +08
å®Œæ•´æ—¥å¿—å°†ä¿å­˜åˆ°: ./testlogs/qwen25_vl_7b_20250904_030423.log
=========================================

>>> æ¨¡å‹è·¯å¾„: Qwen/Qwen2.5-VL-7B-Instruct
>>> å¼€å§‹è¯„æµ‹ VideoMME...
>>> å¼€å§‹æ—¶é—´: Thu 04 Sep 2025 03:04:23 AM +08
The following values were not passed to `accelerate launch` and had defaults used instead:
	`--num_machines` was set to a value of `1`
	`--mixed_precision` was set to a value of `'no'`
	`--dynamo_backend` was set to a value of `'no'`
To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.
[2025-09-04 03:04:30,887] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
/home/minami/miniconda/envs/longva/lib/python3.10/site-packages/deepspeed/runtime/zero/linear.py:49: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.
  def forward(ctx, input, weight, bias=None):
/home/minami/miniconda/envs/longva/lib/python3.10/site-packages/deepspeed/runtime/zero/linear.py:67: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
  def backward(ctx, grad_output):
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.5
[93m [WARNING] [0m using untested triton version (3.1.0), only 1.0.0 is known to be compatible
[32m2025-09-04 03:04:32.415[0m | [1mINFO    [0m | [36m__main__[0m:[36mcli_evaluate[0m:[36m295[0m - [1mVerbosity set to INFO[0m
[32m2025-09-04 03:04:34.072[0m | [33m[1mWARNING [0m | [36mlmms_eval.tasks[0m:[36m_get_task_and_group[0m:[36m473[0m - [33m[1mThe tag coco_karpathy is already registered as a group, this tag will not be registered. This may affect tasks you want to call.[0m
[32m2025-09-04 03:04:34.647[0m | [1mINFO    [0m | [36m__main__[0m:[36mcli_evaluate_single[0m:[36m378[0m - [1mEvaluation tracker args: {'output_path': './testlogs/qwen25_vl_7b_videomme.json'}[0m
[32m2025-09-04 03:04:34.648[0m | [1mINFO    [0m | [36m__main__[0m:[36mcli_evaluate_single[0m:[36m467[0m - [1mSelected Tasks: ['videomme'][0m
[32m2025-09-04 03:04:34.649[0m | [1mINFO    [0m | [36mlmms_eval.evaluator[0m:[36msimple_evaluate[0m:[36m155[0m - [1mSetting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234[0m
Fetching 24 files:   0%|          | 0/24 [00:00<?, ?it/s]Fetching 24 files: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 24/24 [00:00<00:00, 5789.24it/s]
[32m2025-09-04 03:04:38.788[0m | [33m[1mWARNING [0m | [36mlmms_eval.models.qwen2_vl[0m:[36m<module>[0m:[36m25[0m - [33m[1mFailed to import qwen_vl_utils; Please install it via `pip install qwen-vl-utils`[0m
You are using a model of type qwen2_5_vl to instantiate a model of type qwen2_vl. This is not supported for all configurations of models and can yield errors.
Fetching 5 files:   0%|          | 0/5 [00:00<?, ?it/s]Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`
Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`
Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`
Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`
Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`
Fetching 5 files:  20%|â–ˆâ–ˆ        | 1/5 [02:24<09:39, 144.92s/it]Fetching 5 files: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [02:24<00:00, 28.98s/it] 
Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]
Traceback (most recent call last):
  File "/disk3/minami/Vision-Retrieval-Head/lmms-eval/lmms_eval/__main__.py", line 330, in cli_evaluate
    results, samples = cli_evaluate_single(args)
  File "/disk3/minami/Vision-Retrieval-Head/lmms-eval/lmms_eval/__main__.py", line 471, in cli_evaluate_single
    results = evaluator.simple_evaluate(
  File "/disk3/minami/Vision-Retrieval-Head/lmms-eval/lmms_eval/utils.py", line 533, in _wrapper
    return fn(*args, **kwargs)
  File "/disk3/minami/Vision-Retrieval-Head/lmms-eval/lmms_eval/evaluator.py", line 176, in simple_evaluate
    lm = lmms_eval.models.get_model(model).create_from_arg_string(
  File "/disk3/minami/Vision-Retrieval-Head/lmms-eval/lmms_eval/api/model.py", line 110, in create_from_arg_string
    return cls(**args, **args2)
  File "/disk3/minami/Vision-Retrieval-Head/lmms-eval/lmms_eval/models/qwen2_vl.py", line 74, in __init__
    self._model = Qwen2VLForConditionalGeneration.from_pretrained(pretrained, torch_dtype="auto", device_map=self.device_map).eval()
  File "/home/minami/miniconda/envs/longva/lib/python3.10/site-packages/transformers/modeling_utils.py", line 279, in _wrapper
    return func(*args, **kwargs)
  File "/home/minami/miniconda/envs/longva/lib/python3.10/site-packages/transformers/modeling_utils.py", line 4399, in from_pretrained
    ) = cls._load_pretrained_model(
  File "/home/minami/miniconda/envs/longva/lib/python3.10/site-packages/transformers/modeling_utils.py", line 4833, in _load_pretrained_model
    disk_offload_index, cpu_offload_index = _load_state_dict_into_meta_model(
  File "/home/minami/miniconda/envs/longva/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
  File "/home/minami/miniconda/envs/longva/lib/python3.10/site-packages/transformers/modeling_utils.py", line 824, in _load_state_dict_into_meta_model
    _load_parameter_into_model(model, param_name, param.to(param_device))
  File "/home/minami/miniconda/envs/longva/lib/python3.10/site-packages/transformers/modeling_utils.py", line 712, in _load_parameter_into_model
    module.load_state_dict({param_type: tensor}, strict=False, assign=True)
  File "/home/minami/miniconda/envs/longva/lib/python3.10/site-packages/torch/nn/modules/module.py", line 2584, in load_state_dict
    raise RuntimeError(
RuntimeError: Error(s) in loading state_dict for Linear:
	size mismatch for bias: copying a param with shape torch.Size([3584]) from checkpoint, the shape in current model is torch.Size([1280]).
[32m2025-09-04 03:07:15.834[0m | [31m[1mERROR   [0m | [36m__main__[0m:[36mcli_evaluate[0m:[36m349[0m - [31m[1mError during evaluation: Error(s) in loading state_dict for Linear:
	size mismatch for bias: copying a param with shape torch.Size([3584]) from checkpoint, the shape in current model is torch.Size([1280]).. Please set `--verbosity=DEBUG` to get more information.[0m
>>> è¯„æµ‹æˆåŠŸ
>>> ç»“æœä¿å­˜åˆ°: ./testlogs/qwen25_vl_7b_videomme.json

=========================================
è¯„æµ‹å®Œæˆ - Thu 04 Sep 2025 03:07:17 AM +08
ç»“æœæ–‡ä»¶: ./testlogs/qwen25_vl_7b_videomme.json
å®Œæ•´æ—¥å¿—: ./testlogs/qwen25_vl_7b_20250904_030423.log
=========================================

æ—¥å¿—æ–‡ä»¶ä¿¡æ¯ï¼š
-rw-rw-r-- 1 minami minami  14K Sep  4 03:03 ./testlogs/qwen25_vl_7b_20250904_030349.log
-rw-rw-r-- 1 minami minami 7.8K Sep  4 03:07 ./testlogs/qwen25_vl_7b_20250904_030423.log
